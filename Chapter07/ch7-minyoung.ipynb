{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "단층 퍼셉트론을 통한 AND gate 구현"
      ],
      "metadata": {
        "id": "apIKXayRQeHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MnhGBnMgvXh"
      },
      "outputs": [],
      "source": [
        "def AND_gate(x1, x2):\n",
        "    w1 = 0.5 #가중치1\n",
        "    w2 = 0.5 #가중치2\n",
        "    b = -0.7 #편향값\n",
        "    result = x1*w1 + x2*w2 + b\n",
        "    if result <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#AND gate에 입력값 넣어보기\n",
        "AND_gate(0, 0), AND_gate(0, 1), AND_gate(1, 0), AND_gate(1, 1) #두 개의 입력값이 1인 경우에만 1을 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgK1CRbBRcJu",
        "outputId": "d2d3190e-0f00-45e0-c3f1-3c7214f35c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단층 퍼셉트론을 통한 NAND gate 구현"
      ],
      "metadata": {
        "id": "5My-vTiBSGHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NAND_gate(x1, x2):\n",
        "    w1 = -0.5 #가중치1\n",
        "    w2 = -0.5 #가중치2\n",
        "    b = 0.7 #편향값\n",
        "    result = x1*w1 + x2*w2 + b\n",
        "    if result <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "8nq8xio3RpTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NAND gate에 입력값 넣어보기\n",
        "NAND_gate(0, 0), NAND_gate(0, 1), NAND_gate(1, 0), NAND_gate(1, 1) #AND 게이트와 정확히 반대의 결과가 출력됨(두 개의 입력값이 1인 경우에만 1 출력)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x73PObBSQPZ",
        "outputId": "0565e322-6e9b-45ae-d9da-2de8a8a6a054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 1, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단층 퍼셉트론을 통한 OR gate 구현"
      ],
      "metadata": {
        "id": "TVaC9A6mS2AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OR_gate(x1, x2):\n",
        "    w1 = 0.6 #가중치1\n",
        "    w2 = 0.6 #가중치2\n",
        "    b = -0.5 #편향값\n",
        "    result = x1*w1 + x2*w2 + b\n",
        "    if result <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "1JqzWr6SSi59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OR gate에 입력값 넣어보기\n",
        "OR_gate(0, 0), OR_gate(0, 1), OR_gate(1, 0), OR_gate(1, 1) #두 개의 입력이 모두 0인 경우에만 출력값이 0이 나옴"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbgnTn06TAYe",
        "outputId": "f6176fdd-e8be-4b3f-c673-8d5c8200f3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1, 1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다층 퍼셉트론을 통한 XOR gate 구현"
      ],
      "metadata": {
        "id": "a06cugjrW0vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def XOR_gate(x1, x2):\n",
        "  return AND_gate(NAND_gate(x1, x2),OR_gate(x1,x2))"
      ],
      "metadata": {
        "id": "u3cJV5CGTWbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XOR gate에 입력값 넣어보기\n",
        "XOR_gate(0, 0), XOR_gate(0, 1), XOR_gate(1, 0), XOR_gate(1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oPh07MDdamN",
        "outputId": "86b1a693-e80b-44ae-c738-a29e43ceba50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1, 1, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력의 차원이 3, 출력의 차원이 2인 위 인공 신경망을 구현"
      ],
      "metadata": {
        "id": "8u-iYE8W86IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 3개의 입력과 2개의 출력\n",
        "model.add(Dense(2, input_dim=3, activation='softmax'))"
      ],
      "metadata": {
        "id": "ZnfLdSdwdp7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다층 퍼셉트론 구현"
      ],
      "metadata": {
        "id": "K7O_Vrgp9K5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 4개의 입력과 8개의 출력\n",
        "model.add(Dense(8, input_dim=4, activation='relu'))\n",
        "\n",
        "# 이어서 8개의 출력\n",
        "model.add(Dense(8, activation='relu'))\n",
        "\n",
        "# 이어서 3개의 출력\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "metadata": {
        "id": "PC0BTihH9FFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스에서 드롭 아웃 모델 추가하기"
      ],
      "metadata": {
        "id": "6v7j7HlTYJbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Dense\n",
        "\n",
        "max_words = 10000\n",
        "num_classes = 46\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(max_words,), activation='relu')) #256개의 뉴런을 가진 완전 연결 층(Dense)을 추가,input_shape 매개변수를 통해 입력 데이터의 형태를 지정, 이 모델은 max_words(10,000) 개의 단어로 구성된 입력을 받음, 활성화 함수로 'relu'를 사용\n",
        "model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\n",
        "model.add(Dense(num_classes, activation='softmax'))#활성화 함수로 'softmax'를 사용하여 다중 클래스 분류 문제에서 각 클래스에 속할 확률을 출력.\n"
      ],
      "metadata": {
        "id": "sSw5rn389Wad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스-전처리, Tokenizer(): 토큰화와 정수 인코딩"
      ],
      "metadata": {
        "id": "bNiiX9lJ14Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer() #Tokenizer 객체 생성\n",
        "train_text = \"The earth is an awesome place live\" #train_text 변수에 문자열 저장\n",
        "\n",
        "# 단어 집합 생성-> 토큰화하고 각 단어에 고유한 정수 인덱스 생성\n",
        "tokenizer.fit_on_texts([train_text])\n",
        "\n",
        "# 정수 인코딩\n",
        "sub_text = \"The earth is an great place live\" #sub_text 변수열에 문자열 저장\n",
        "sequences = tokenizer.texts_to_sequences([sub_text])[0] # 이미 생성된 단어 집합을 기반으로 입력 텍스트를 정수 시퀀스로 변환\n",
        "\n",
        "print(\"정수 인코딩 : \",sequences)\n",
        "print(\"단어 집합 : \",tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKdw_qunZ23f",
        "outputId": "a697e002-a160-46c8-a96a-b2c4db9c143a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 :  [1, 2, 3, 4, 6, 7]\n",
            "단어 집합 :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스-전처리, pad_sequence(): 모든 샘플의 길이를 동일하게 맞추어줌"
      ],
      "metadata": {
        "id": "ANKkhxHT3Gf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='pre') #첫번째 인자: 패딩을 진행할 데이터 maxlen: 정규화할 길이 padding: 'pre'를 선택하면 앞에 0을 채우고 'post'를 선택하면 뒤에 0을 채움"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnOl1U0S2D3-",
        "outputId": "9fe1b5b8-9c86-4d44-f362-555f0a48b3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [0, 7, 8]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스-모델링"
      ],
      "metadata": {
        "id": "8_gMzLPR53l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential() #층을 구성\n",
        "model.add(Dense(8, input_dim=4, activation='relu')) #은닉층\n",
        "model.add(Dense(1, activation='sigmoid')) # 출력층\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IhYvKZA3dok",
        "outputId": "c46de0a9-22f5-4045-9061-fb1b5b53f23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 8)                 40        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49 (196.00 Byte)\n",
            "Trainable params: 49 (196.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스-컴파일"
      ],
      "metadata": {
        "id": "FuuicAKn7mL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "vocab_size = 10000\n",
        "embedding_dim = 32\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim)) #입력 텍스트의 각 단어를 고정된 크기의 벡터로 변환\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid')) #시그모이드 활성화 함수를 사용하여 이진 분류 문제에 대한 확률을 출력\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) #optimizer로 'rmsprop'를 사용, RMSprop 옵티마이저를 선택.loss로 'binary_crossentropy'를 사용하여 이진 분류 문제에 대한 손실을 정의.metrics로 'acc'를 사용하여 정확도를 모니터링"
      ],
      "metadata": {
        "id": "HlB-dh--66ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스의 함수형 API(Sequential API와의 차이 알아보기)"
      ],
      "metadata": {
        "id": "4iY6_BBPaiFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sequential API로 만든 모델"
      ],
      "metadata": {
        "id": "Hf1pddCpau2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(3, input_dim=4, activation='softmax')) #입력 차원 4, 출력 차원3, 활성화 함수는 소프트맥스 함수 사용"
      ],
      "metadata": {
        "id": "4Xu422Ni8XCR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-1. Functional API로 FFNN 만들기"
      ],
      "metadata": {
        "id": "j_4CDyVgbZJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(10,)) #10개의 입력층 정의\n",
        "x = Dense(8, activation=\"relu\")(inputs) #8개의 뉴런과 ReLU 활성화 함수를 가진 첫 번째 은닉층 정의, input을 입력으로 받음\n",
        "x = Dense(4, activation=\"relu\")(x) #4개의 뉴런과 ReLU 활성화 함수를 가진 두 번째 은닉층 정의, input을 x로 받음\n",
        "x = Dense(1, activation=\"linear\")(x) #1개의 뉴런과 선형 활성화 함수를 가진 출력층 정의, input을 x로 받음\n",
        "model = Model(inputs, x) #입력과 출력을 지정하여 모델 정의\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) #옵티마이저로 'rmsprop'을 사용하고, 손실 함수로 'categorical_crossentropy'를 사용, 모델의 평가 지표로 'accuracy'를 설정\n",
        "# model.fit(data, labels) 실제 데이터를 로드하고 모델을 학습시키는 부분"
      ],
      "metadata": {
        "id": "n5WBM12ebUri"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-2 Functional API로 선형 회귀 만들기"
      ],
      "metadata": {
        "id": "V-Dhyl85fPlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "X = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\n",
        "y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\n",
        "\n",
        "inputs = Input(shape=(1,)) #1개의 입력층 정의\n",
        "output = Dense(1, activation='linear')(inputs) #1개의 뉴런과 선형 활성화 함수를 가진 출력층 정의, input을 x로 받음\n",
        "linear_model = Model(inputs, output) #입력과 출력을 지정하여 모델 정의\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.01) #확률적 경사 하강법(SGD) 옵티마이저를 정의, 학습률(learning rate)은 0.01로 설정\n",
        "\n",
        "linear_model.compile(optimizer=sgd, loss='mse', metrics=['mse']) #옵티마이저로 SGD를 사용, 손실 함수로 MSE를 사용하며 최소화되어야하는 대상\n",
        "linear_model.fit(X, y, epochs=300) #에포크 수를 300으로 설정하여 모델을 학습"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShQE0ghWbucV",
        "outputId": "11340dde-dfd3-4eec-ca0e-ed91ece144c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 1s 735ms/step - loss: 2634.0308 - mse: 2634.0308\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 325.0948 - mse: 325.0948\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 40.9971 - mse: 40.9971\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 6.0403 - mse: 6.0403\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.7387 - mse: 1.7387\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.2088 - mse: 1.2088\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1431 - mse: 1.1431\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1345 - mse: 1.1345\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1329 - mse: 1.1329\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1322 - mse: 1.1322\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1316 - mse: 1.1316\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1310 - mse: 1.1310\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1304 - mse: 1.1304\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1298 - mse: 1.1298\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1292 - mse: 1.1292\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1287 - mse: 1.1287\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1281 - mse: 1.1281\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1275 - mse: 1.1275\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1270 - mse: 1.1270\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1264 - mse: 1.1264\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1259 - mse: 1.1259\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1254 - mse: 1.1254\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1248 - mse: 1.1248\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1243 - mse: 1.1243\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1238 - mse: 1.1238\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1232 - mse: 1.1232\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1227 - mse: 1.1227\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1222 - mse: 1.1222\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1217 - mse: 1.1217\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1212 - mse: 1.1212\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1207 - mse: 1.1207\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1202 - mse: 1.1202\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1197 - mse: 1.1197\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1192 - mse: 1.1192\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1187 - mse: 1.1187\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1182 - mse: 1.1182\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1177 - mse: 1.1177\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1173 - mse: 1.1173\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1168 - mse: 1.1168\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1163 - mse: 1.1163\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1159 - mse: 1.1159\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1154 - mse: 1.1154\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1150 - mse: 1.1150\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1145 - mse: 1.1145\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1141 - mse: 1.1141\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1136 - mse: 1.1136\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1132 - mse: 1.1132\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1127 - mse: 1.1127\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1123 - mse: 1.1123\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1119 - mse: 1.1119\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1115 - mse: 1.1115\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1110 - mse: 1.1110\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1106 - mse: 1.1106\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1102 - mse: 1.1102\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1098 - mse: 1.1098\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1094 - mse: 1.1094\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1090 - mse: 1.1090\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1086 - mse: 1.1086\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1082 - mse: 1.1082\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1078 - mse: 1.1078\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1074 - mse: 1.1074\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1070 - mse: 1.1070\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1066 - mse: 1.1066\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1062 - mse: 1.1062\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1058 - mse: 1.1058\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1055 - mse: 1.1055\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1051 - mse: 1.1051\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1047 - mse: 1.1047\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1043 - mse: 1.1043\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1040 - mse: 1.1040\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1036 - mse: 1.1036\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1033 - mse: 1.1033\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1029 - mse: 1.1029\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1026 - mse: 1.1026\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1022 - mse: 1.1022\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1019 - mse: 1.1019\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1015 - mse: 1.1015\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1012 - mse: 1.1012\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1008 - mse: 1.1008\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1005 - mse: 1.1005\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1002 - mse: 1.1002\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0998 - mse: 1.0998\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0995 - mse: 1.0995\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0992 - mse: 1.0992\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0989 - mse: 1.0989\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0985 - mse: 1.0985\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0982 - mse: 1.0982\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0979 - mse: 1.0979\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0976 - mse: 1.0976\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0973 - mse: 1.0973\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0970 - mse: 1.0970\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0967 - mse: 1.0967\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0964 - mse: 1.0964\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0961 - mse: 1.0961\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.0958 - mse: 1.0958\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0955 - mse: 1.0955\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0952 - mse: 1.0952\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0949 - mse: 1.0949\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0946 - mse: 1.0946\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0943 - mse: 1.0943\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.0940 - mse: 1.0940\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0938 - mse: 1.0938\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0935 - mse: 1.0935\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0932 - mse: 1.0932\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0929 - mse: 1.0929\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0927 - mse: 1.0927\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0924 - mse: 1.0924\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0921 - mse: 1.0921\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0919 - mse: 1.0919\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0916 - mse: 1.0916\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0914 - mse: 1.0914\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0911 - mse: 1.0911\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0908 - mse: 1.0908\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0906 - mse: 1.0906\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0903 - mse: 1.0903\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0901 - mse: 1.0901\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0898 - mse: 1.0898\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0896 - mse: 1.0896\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0893 - mse: 1.0893\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0891 - mse: 1.0891\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0889 - mse: 1.0889\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0886 - mse: 1.0886\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0884 - mse: 1.0884\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0882 - mse: 1.0882\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0879 - mse: 1.0879\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0877 - mse: 1.0877\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0875 - mse: 1.0875\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0872 - mse: 1.0872\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0870 - mse: 1.0870\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0868 - mse: 1.0868\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0866 - mse: 1.0866\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0864 - mse: 1.0864\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0861 - mse: 1.0861\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0859 - mse: 1.0859\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0857 - mse: 1.0857\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0855 - mse: 1.0855\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0853 - mse: 1.0853\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0851 - mse: 1.0851\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0849 - mse: 1.0849\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0847 - mse: 1.0847\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0845 - mse: 1.0845\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0843 - mse: 1.0843\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0841 - mse: 1.0841\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0839 - mse: 1.0839\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0837 - mse: 1.0837\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0835 - mse: 1.0835\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0833 - mse: 1.0833\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0831 - mse: 1.0831\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0829 - mse: 1.0829\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0827 - mse: 1.0827\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0825 - mse: 1.0825\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0823 - mse: 1.0823\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0821 - mse: 1.0821\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0820 - mse: 1.0820\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0818 - mse: 1.0818\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0816 - mse: 1.0816\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0814 - mse: 1.0814\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0812 - mse: 1.0812\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0811 - mse: 1.0811\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0809 - mse: 1.0809\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0807 - mse: 1.0807\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0806 - mse: 1.0806\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0804 - mse: 1.0804\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0802 - mse: 1.0802\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0801 - mse: 1.0801\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0799 - mse: 1.0799\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0797 - mse: 1.0797\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0796 - mse: 1.0796\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0794 - mse: 1.0794\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0792 - mse: 1.0792\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0791 - mse: 1.0791\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0789 - mse: 1.0789\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0788 - mse: 1.0788\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0786 - mse: 1.0786\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0785 - mse: 1.0785\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0783 - mse: 1.0783\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0782 - mse: 1.0782\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0780 - mse: 1.0780\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0779 - mse: 1.0779\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0777 - mse: 1.0777\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0776 - mse: 1.0776\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0774 - mse: 1.0774\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0773 - mse: 1.0773\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0771 - mse: 1.0771\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0770 - mse: 1.0770\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0769 - mse: 1.0769\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0767 - mse: 1.0767\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0766 - mse: 1.0766\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0764 - mse: 1.0764\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0763 - mse: 1.0763\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0762 - mse: 1.0762\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0760 - mse: 1.0760\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0759 - mse: 1.0759\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0758 - mse: 1.0758\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0756 - mse: 1.0756\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0755 - mse: 1.0755\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0754 - mse: 1.0754\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0753 - mse: 1.0753\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0751 - mse: 1.0751\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0750 - mse: 1.0750\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0749 - mse: 1.0749\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0748 - mse: 1.0748\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0746 - mse: 1.0746\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0745 - mse: 1.0745\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0744 - mse: 1.0744\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0743 - mse: 1.0743\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0742 - mse: 1.0742\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0740 - mse: 1.0740\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0739 - mse: 1.0739\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0738 - mse: 1.0738\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0737 - mse: 1.0737\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0736 - mse: 1.0736\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0735 - mse: 1.0735\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0734 - mse: 1.0734\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0732 - mse: 1.0732\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0731 - mse: 1.0731\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0730 - mse: 1.0730\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0729 - mse: 1.0729\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0728 - mse: 1.0728\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0727 - mse: 1.0727\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0726 - mse: 1.0726\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0725 - mse: 1.0725\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0724 - mse: 1.0724\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0723 - mse: 1.0723\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0722 - mse: 1.0722\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0721 - mse: 1.0721\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0720 - mse: 1.0720\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.0719 - mse: 1.0719\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0718 - mse: 1.0718\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0717 - mse: 1.0717\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0716 - mse: 1.0716\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0715 - mse: 1.0715\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0714 - mse: 1.0714\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0713 - mse: 1.0713\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0712 - mse: 1.0712\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0711 - mse: 1.0711\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0710 - mse: 1.0710\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0709 - mse: 1.0709\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0709 - mse: 1.0709\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0708 - mse: 1.0708\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0707 - mse: 1.0707\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0706 - mse: 1.0706\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0705 - mse: 1.0705\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0704 - mse: 1.0704\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0703 - mse: 1.0703\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0702 - mse: 1.0702\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0702 - mse: 1.0702\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.0701 - mse: 1.0701\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0700 - mse: 1.0700\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0699 - mse: 1.0699\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0698 - mse: 1.0698\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0697 - mse: 1.0697\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0697 - mse: 1.0697\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0696 - mse: 1.0696\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0695 - mse: 1.0695\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0694 - mse: 1.0694\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0693 - mse: 1.0693\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0693 - mse: 1.0693\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.0692 - mse: 1.0692\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0691 - mse: 1.0691\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0690 - mse: 1.0690\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0690 - mse: 1.0690\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0689 - mse: 1.0689\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0688 - mse: 1.0688\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0687 - mse: 1.0687\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0687 - mse: 1.0687\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0686 - mse: 1.0686\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0685 - mse: 1.0685\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0684 - mse: 1.0684\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0684 - mse: 1.0684\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0683 - mse: 1.0683\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0682 - mse: 1.0682\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0682 - mse: 1.0682\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.0681 - mse: 1.0681\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0680 - mse: 1.0680\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0680 - mse: 1.0680\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0679 - mse: 1.0679\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0678 - mse: 1.0678\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0678 - mse: 1.0678\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0677 - mse: 1.0677\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0676 - mse: 1.0676\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0676 - mse: 1.0676\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0675 - mse: 1.0675\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0675 - mse: 1.0675\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0674 - mse: 1.0674\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0673 - mse: 1.0673\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0673 - mse: 1.0673\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0672 - mse: 1.0672\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0671 - mse: 1.0671\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0671 - mse: 1.0671\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0670 - mse: 1.0670\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0670 - mse: 1.0670\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0669 - mse: 1.0669\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0669 - mse: 1.0669\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0668 - mse: 1.0668\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0667 - mse: 1.0667\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0667 - mse: 1.0667\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0666 - mse: 1.0666\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0666 - mse: 1.0666\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0665 - mse: 1.0665\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ec1e00112a0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02-3 Functional API로 로지스틱 회귀 만들기"
      ],
      "metadata": {
        "id": "-li4Iw2ZggNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(3,)) #3개의 입력층 정의\n",
        "output = Dense(1, activation='sigmoid')(inputs) #1개의 뉴런과 시그모이드 함수를 활성화 함수로 가진 출력층 정의, inputs을 입력으로 받음\n",
        "logistic_model = Model(inputs, output) #입력과 출력 지정하여 모델 정의"
      ],
      "metadata": {
        "id": "6HJFC_JUgVJo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "02-4 Functional API로 다중 입력을 받는 모델 만들기"
      ],
      "metadata": {
        "id": "4UMx2MEkg7R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 두 개의 입력층을 정의\n",
        "inputA = Input(shape=(64,))\n",
        "inputB = Input(shape=(128,))\n",
        "\n",
        "# 첫번째 입력층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
        "x = Dense(16, activation=\"relu\")(inputA)\n",
        "x = Dense(8, activation=\"relu\")(x)\n",
        "x = Model(inputs=inputA, outputs=x)\n",
        "\n",
        "# 두번째 입력층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
        "y = Dense(64, activation=\"relu\")(inputB)\n",
        "y = Dense(32, activation=\"relu\")(y)\n",
        "y = Dense(8, activation=\"relu\")(y)\n",
        "y = Model(inputs=inputB, outputs=y)\n",
        "\n",
        "# 두개의 인공 신경망의 출력을 연결(concatenate)\n",
        "result = concatenate([x.output, y.output]) #두개의 모델이 합쳐져 하나의 벡터가 됨\n",
        "\n",
        "z = Dense(2, activation=\"relu\")(result)\n",
        "z = Dense(1, activation=\"linear\")(z)\n",
        "\n",
        "model = Model(inputs=[x.input, y.input], outputs=z) #최종적인 모델로, 두 개의 입력 inputA와 inputB를 받고 z 모델로부터 출력을 생성"
      ],
      "metadata": {
        "id": "wOZY97eng6j2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "02-5 Functional API로 RNN 은닉층을 가지는 모델 만들기"
      ],
      "metadata": {
        "id": "3EWiKOvbho-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(50,1)) #모델의 입력을 정의. 이 입력은 시퀀스 길이가 50이고 각 시점마다 1개의 특성을 가진 시퀀스 데이터를 받음\n",
        "lstm_layer = LSTM(10)(inputs) # LSTM 레이어를 정의. 10개의 LSTM 뉴런을 사용하고, 입력으로부터 시퀀스 정보를 추출\n",
        "x = Dense(10, activation='relu')(lstm_layer) #LSTM 레이어의 출력을 입력으로 받는 밀집(Dense) 레이어를 정의\n",
        "output = Dense(1, activation='sigmoid')(x) #1개의 뉴런과 시그모이드 활성화 함수를 사용하는 출력층 정의\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output) #입력과 출력을 지정하여 모델 정의"
      ],
      "metadata": {
        "id": "TF09MGZmhl98"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다층 퍼셉트론(MultiLayer Perceptron, MLP)으로 텍스트 분류하기"
      ],
      "metadata": {
        "id": "e4mq25xGotgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스의 texts_to_matrix() 이해하기"
      ],
      "metadata": {
        "id": "BuI2KgxRo01U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "texts = ['먹고 싶은 사과', '먹고 싶은 바나나', '길고 노란 바나나 바나나', '저는 과일이 좋아요']\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts) #텍스트 데이터에 대해서 정수 인코딩\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YptgxzrKijOv",
        "outputId": "98d72c7b-0dc0-449f-9093-86c9e70aea9b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'바나나': 1, '먹고': 2, '싶은': 3, '사과': 4, '길고': 5, '노란': 6, '저는': 7, '과일이': 8, '좋아요': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_matrix(texts, mode = 'count')) # texts_to_matrix의 입력으로 texts를 넣고, 모드는 'count'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47JmHLaCpIu5",
        "outputId": "e25a7e8a-4485-4a46-893b-804503cebb0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 2. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_matrix(texts, mode = 'binary')) #해당 단어가 존재하는지만 관심을 가지고 해당 단어가 몇 개였는지는 무시"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU7bDj61pJXp",
        "outputId": "8382b771-3fd2-4530-cf37-80e21d80e15b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_matrix(texts, mode = 'tfidf').round(2)) # 둘째 자리까지 반올림하여 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVgffNnfpzhp",
        "outputId": "8b74e0d8-f200-491a-f55d-2cd0bd555ca7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.   0.85 0.85 1.1  0.   0.   0.   0.   0.  ]\n",
            " [0.   0.85 0.85 0.85 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   1.43 0.   0.   0.   1.1  1.1  0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   1.1  1.1  1.1 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_matrix(texts, mode = 'freq').round(2)) # 둘째 자리까지 반올림하여 출력,  각 문서의 크기(각 문서에서 등장한 모든 단어의 개수의 총 합)를 분모로 하는 표현 방법"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RZzjasip1fC",
        "outputId": "9a37e30d-6a12-4959-f644-3b5b46b21a1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.   0.33 0.33 0.33 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.33 0.33 0.33 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.5  0.   0.   0.   0.25 0.25 0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.33 0.33 0.33]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20개 뉴스 그룹(Twenty Newsgroups) 데이터에 대한 이해"
      ],
      "metadata": {
        "id": "IrHeiw30qH2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "newsdata = fetch_20newsgroups(subset = 'train') # 'train'을 기재하면 훈련 데이터만 리턴한다."
      ],
      "metadata": {
        "id": "GEswPR4Np7UD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsdata.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpWNIkHIqLN6",
        "outputId": "99d0aada-3710-4197-f0d4-e163e005741d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 샘플의 개수 : {}'.format(len(newsdata.data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmeVvRgpqWaC",
        "outputId": "5f0cfdc6-7e52-4890-9270-e0cc6c1f56d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련용 샘플의 개수 : 11314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('총 주제의 개수 : {}'.format(len(newsdata.target_names)))\n",
        "print(newsdata.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C98K6GamqbcR",
        "outputId": "aa489046-74d7-4c31-82da-8fd91193b389"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 주제의 개수 : 20\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('첫번째 샘플의 레이블 : {}'.format(newsdata.target[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMOtXnIwqhki",
        "outputId": "ae629fe1-46a0-4a42-9127-c1ec45492f11"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫번째 샘플의 레이블 : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('7번 레이블이 의미하는 주제 : {}'.format(newsdata.target_names[7]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdWeK35cqk2O",
        "outputId": "f47b9146-115d-4eca-c5ea-d7f9e1773148"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7번 레이블이 의미하는 주제 : rec.autos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsdata.data[0]) # 첫번째 샘플 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNBR3rz7qnHJ",
        "outputId": "436045dd-ccc5-4704-abf4-3ad31c974f9f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(newsdata.data, columns = ['email']) #data로부터 데이터프레임을 생성하고, target 열을 추가한 뒤에 상위 5개의 행을 출력\n",
        "data['target'] = pd.Series(newsdata.target)\n",
        "data[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "P-QoWBSbqr1q",
        "outputId": "7c106cca-85da-4f4d-e8e6-14ccc8702e0d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               email  target\n",
              "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7\n",
              "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4\n",
              "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4\n",
              "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1\n",
              "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9124b938-a305-4aad-88eb-afe09cd4c965\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>email</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9124b938-a305-4aad-88eb-afe09cd4c965')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9124b938-a305-4aad-88eb-afe09cd4c965 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9124b938-a305-4aad-88eb-afe09cd4c965');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cf4e4230-2b44-4a0a-ba58-20be16fa1cdd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cf4e4230-2b44-4a0a-ba58-20be16fa1cdd')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cf4e4230-2b44-4a0a-ba58-20be16fa1cdd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('중복을 제외한 샘플의 수 : {}'.format(data['email'].nunique()))\n",
        "print('중복을 제외한 주제의 수 : {}'.format(data['target'].nunique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Lu4AzuqzPC",
        "outputId": "b26906af-e588-4b9e-d832-251af32e1736"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "중복을 제외한 샘플의 수 : 11314\n",
            "중복을 제외한 주제의 수 : 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['target'].value_counts().plot(kind='bar'); #레이블 값의 분포를 시각화"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "2SfidemVrALG",
        "outputId": "9f940a47-fd7e-46f2-f6f1-de31b0a6e76b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.groupby('target').size().reset_index(name='count')) #각 레이블이 몇 개 있는지 구체적인 수치로 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8aWuDhbrEUe",
        "outputId": "b0005e44-3c50-4a91-edb9-51257a11d7d4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    target  count\n",
            "0        0    480\n",
            "1        1    584\n",
            "2        2    591\n",
            "3        3    590\n",
            "4        4    578\n",
            "5        5    593\n",
            "6        6    585\n",
            "7        7    594\n",
            "8        8    598\n",
            "9        9    597\n",
            "10      10    600\n",
            "11      11    595\n",
            "12      12    591\n",
            "13      13    594\n",
            "14      14    593\n",
            "15      15    599\n",
            "16      16    546\n",
            "17      17    564\n",
            "18      18    465\n",
            "19      19    377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsdata_test = fetch_20newsgroups(subset='test', shuffle=True) #훈련 데이터와 테스트 데이터의 본문과 레이블을 각각 저장\n",
        "train_email = data['email']\n",
        "train_label = data['target']\n",
        "test_email = newsdata_test.data\n",
        "test_label = newsdata_test.target"
      ],
      "metadata": {
        "id": "pMphgGSLrIQR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "num_classes = 20"
      ],
      "metadata": {
        "id": "EsqGGpZgrVm7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(train_data, test_data, mode): # 전처리 함수\n",
        "    tokenizer = Tokenizer(num_words = vocab_size) # vocab_size 개수만큼의 단어만 사용한다.\n",
        "    tokenizer.fit_on_texts(train_data)\n",
        "    X_train = tokenizer.texts_to_matrix(train_data, mode=mode) # 샘플 수 × vocab_size 크기의 행렬 생성\n",
        "    X_test = tokenizer.texts_to_matrix(test_data, mode=mode) # 샘플 수 × vocab_size 크기의 행렬 생성\n",
        "    return X_train, X_test, tokenizer.index_word"
      ],
      "metadata": {
        "id": "zvmfJCIQrYWE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, index_to_word = prepare_data(train_email, test_email, 'binary') # binary 모드로 변환\n",
        "y_train = to_categorical(train_label, num_classes) # 원-핫 인코딩\n",
        "y_test = to_categorical(test_label, num_classes) # 원-핫 인코딩"
      ],
      "metadata": {
        "id": "zedymI_UrddU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 샘플 본문의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 본문의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU4F9He3rfvF",
        "outputId": "e3285f67-7e05-4de7-cc12-27738b7d9651"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 샘플 본문의 크기 : (11314, 10000)\n",
            "훈련 샘플 레이블의 크기 : (11314, 20)\n",
            "테스트 샘플 본문의 크기 : (7532, 10000)\n",
            "테스트 샘플 레이블의 크기 : (7532, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('빈도수 상위 1번 단어 : {}'.format(index_to_word[1]))\n",
        "print('빈도수 상위 9999번 단어 : {}'.format(index_to_word[9999]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpQDJYlMrw6t",
        "outputId": "9290a483-1b69-4dc7-ac5d-add57aea2cf3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "빈도수 상위 1번 단어 : the\n",
            "빈도수 상위 9999번 단어 : mic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다층 퍼셉트론(Multilayer Perceptron, MLP)을 사용하여 텍스트 분류하기"
      ],
      "metadata": {
        "id": "M3rk8FDqr1aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "def fit_and_evaluate(X_train, y_train, X_test, y_test):\n",
        "    model = Sequential() #Sequential 모델을 초기화\n",
        "    model.add(Dense(256, input_shape=(vocab_size,), activation='relu')) #첫 번째 레이어는 256개의 뉴런을 가지며, 입력 데이터의 크기인 vocab_size를 입력으로 받고, 활성화 함수로는 ReLU를 사용\n",
        "    model.add(Dropout(0.5)) #50%의 드롭아웃 비율을 사용\n",
        "    model.add(Dense(128, activation='relu')) # 128개의 뉴런과 ReLU 활성화 함수\n",
        "    model.add(Dropout(0.5)) #50%의 드롭아웃 비율을 사용\n",
        "    model.add(Dense(num_classes, activation='softmax')) #num_classes 개의 뉴런을 가지며, 다중 클래스 분류 문제를 다루기 위해 softmax 활성화 함수를 사용\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #손실 함수로는 'categorical_crossentropy'를, 옵티마이저로는 'adam'을 사용하며, 평가 지표로 'accuracy'를 설정\n",
        "    model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.1) #모델을 학습. 주어진 학습 데이터를 사용하고 배치 크기는 128, 에포크는 5로 설정. validation_split을 통해 학습 데이터의 10%를 검증 데이터로 사용\n",
        "    score = model.evaluate(X_test, y_test, batch_size=128, verbose=0) #테스트 데이터에 대한 평가를 수행하고 정확도를 반환\n",
        "    return score[1]"
      ],
      "metadata": {
        "id": "mDcam3dnrzcN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modes = ['binary', 'count', 'tfidf', 'freq'] # 4개의 모드를 리스트에 저장.\n",
        "\n",
        "for mode in modes: # 4개의 모드에 대해서 각각 아래의 작업을 반복한다.\n",
        "    X_train, X_test, _ = prepare_data(train_email, test_email, mode) # 모드에 따라서 데이터를 전처리\n",
        "    score = fit_and_evaluate(X_train, y_train, X_test, y_test) # 모델을 훈련하고 평가.\n",
        "    print(mode+' 모드의 테스트 정확도:', score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCiMzbNyr6Bi",
        "outputId": "78f8a070-8646-40ad-e0da-05aff9144203"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "80/80 [==============================] - 6s 62ms/step - loss: 2.2756 - accuracy: 0.3468 - val_loss: 0.9502 - val_accuracy: 0.8198\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 8s 103ms/step - loss: 0.8597 - accuracy: 0.7659 - val_loss: 0.4713 - val_accuracy: 0.8869\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 5s 58ms/step - loss: 0.4194 - accuracy: 0.8905 - val_loss: 0.3532 - val_accuracy: 0.9037\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.2595 - accuracy: 0.9350 - val_loss: 0.3258 - val_accuracy: 0.9090\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1671 - accuracy: 0.9603 - val_loss: 0.3055 - val_accuracy: 0.9125\n",
            "binary 모드의 테스트 정확도: 0.8331120610237122\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 6s 62ms/step - loss: 2.7942 - accuracy: 0.2258 - val_loss: 1.7117 - val_accuracy: 0.7023\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 1.4934 - accuracy: 0.6161 - val_loss: 0.7266 - val_accuracy: 0.8604\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.8911 - accuracy: 0.7946 - val_loss: 0.5008 - val_accuracy: 0.8710\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.5414 - accuracy: 0.8744 - val_loss: 0.4246 - val_accuracy: 0.8896\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 5s 61ms/step - loss: 0.3912 - accuracy: 0.9149 - val_loss: 0.3731 - val_accuracy: 0.8975\n",
            "count 모드의 테스트 정확도: 0.8203664422035217\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 2.2404 - accuracy: 0.3580 - val_loss: 0.7983 - val_accuracy: 0.8436\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.8557 - accuracy: 0.7668 - val_loss: 0.4186 - val_accuracy: 0.8869\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.4537 - accuracy: 0.8794 - val_loss: 0.3408 - val_accuracy: 0.9064\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.3002 - accuracy: 0.9264 - val_loss: 0.3351 - val_accuracy: 0.9134\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2445 - accuracy: 0.9475 - val_loss: 0.3221 - val_accuracy: 0.9196\n",
            "tfidf 모드의 테스트 정확도: 0.8260754346847534\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 2.9787 - accuracy: 0.0814 - val_loss: 2.9316 - val_accuracy: 0.1714\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 5s 61ms/step - loss: 2.7355 - accuracy: 0.2067 - val_loss: 2.4266 - val_accuracy: 0.4373\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 2.2162 - accuracy: 0.3320 - val_loss: 1.8999 - val_accuracy: 0.5989\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 1.7429 - accuracy: 0.4709 - val_loss: 1.4673 - val_accuracy: 0.6749\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 1.3775 - accuracy: 0.5832 - val_loss: 1.1541 - val_accuracy: 0.7323\n",
            "freq 모드의 테스트 정확도: 0.6994158029556274\n"
          ]
        }
      ]
    }
  ]
}