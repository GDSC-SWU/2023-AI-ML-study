{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366f7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib3\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e65935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수: 227815\n"
     ]
    }
   ],
   "source": [
    "lines=pd.read_csv(\"C:/Users/kimchaeyeon/Desktop/GDSC/fra.txt\", names=['src','tar','lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수:',len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee90f95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16574</th>\n",
       "      <td>It's not a joke.</td>\n",
       "      <td>Ce n'est pas une blague.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49442</th>\n",
       "      <td>I'll give you a call.</td>\n",
       "      <td>Je vous passerai un coup de fil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22595</th>\n",
       "      <td>Stop the car now!</td>\n",
       "      <td>Arrêtez la voiture, maintenant !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58872</th>\n",
       "      <td>May I use your toilet?</td>\n",
       "      <td>Puis-je utiliser vos toilettes ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28081</th>\n",
       "      <td>It's all I've got.</td>\n",
       "      <td>C'est tout ce que j'ai.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12284</th>\n",
       "      <td>It's your book.</td>\n",
       "      <td>C'est ton livre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15333</th>\n",
       "      <td>I grow tomatoes.</td>\n",
       "      <td>Je cultive des tomates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56312</th>\n",
       "      <td>I dislike all of them.</td>\n",
       "      <td>Je les déteste toutes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21071</th>\n",
       "      <td>I think I'm done.</td>\n",
       "      <td>Je pense que j'ai terminé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43229</th>\n",
       "      <td>That isn't possible.</td>\n",
       "      <td>Cela n'est pas possible.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                               tar\n",
       "16574        It's not a joke.          Ce n'est pas une blague.\n",
       "49442   I'll give you a call.  Je vous passerai un coup de fil.\n",
       "22595       Stop the car now!  Arrêtez la voiture, maintenant !\n",
       "58872  May I use your toilet?  Puis-je utiliser vos toilettes ?\n",
       "28081      It's all I've got.           C'est tout ce que j'ai.\n",
       "12284         It's your book.                  C'est ton livre.\n",
       "15333        I grow tomatoes.           Je cultive des tomates.\n",
       "56312  I dislike all of them.            Je les déteste toutes.\n",
       "21071       I think I'm done.        Je pense que j'ai terminé.\n",
       "43229    That isn't possible.          Cela n'est pas possible."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000] # 6만개만 저장\n",
    "lines.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3dafc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22569</th>\n",
       "      <td>Stay out of this.</td>\n",
       "      <td>\\t Restez en dehors de ça ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>You may swim.</td>\n",
       "      <td>\\t Vous pouvez nager. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42184</th>\n",
       "      <td>It was all worth it.</td>\n",
       "      <td>\\t Tout en valait la peine. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32288</th>\n",
       "      <td>Don't you think so?</td>\n",
       "      <td>\\t Ne le penses-tu pas ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29590</th>\n",
       "      <td>This is your fate.</td>\n",
       "      <td>\\t C'est ton sort. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59161</th>\n",
       "      <td>Put your coat back on.</td>\n",
       "      <td>\\t Remets ton manteau. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>Hold this.</td>\n",
       "      <td>\\t Tiens ça ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45431</th>\n",
       "      <td>Why are you running?</td>\n",
       "      <td>\\t Pourquoi courez-vous ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51898</th>\n",
       "      <td>This car is like new.</td>\n",
       "      <td>\\t Cette voiture est comme neuve. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56392</th>\n",
       "      <td>I don't hear anything.</td>\n",
       "      <td>\\t Je n'entends rien du tout. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                   tar\n",
       "22569       Stay out of this.        \\t Restez en dehors de ça ! \\n\n",
       "6917            You may swim.              \\t Vous pouvez nager. \\n\n",
       "42184    It was all worth it.        \\t Tout en valait la peine. \\n\n",
       "32288     Don't you think so?           \\t Ne le penses-tu pas ? \\n\n",
       "29590      This is your fate.                 \\t C'est ton sort. \\n\n",
       "59161  Put your coat back on.             \\t Remets ton manteau. \\n\n",
       "1050               Hold this.                      \\t Tiens ça ! \\n\n",
       "45431    Why are you running?          \\t Pourquoi courez-vous ? \\n\n",
       "51898   This car is like new.  \\t Cette voiture est comme neuve. \\n\n",
       "56392  I don't hear anything.      \\t Je n'entends rien du tout. \\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91f765f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 문자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb2bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 80\n",
      "target 문장의 char 집합 : 104\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print('source 문장의 char 집합 :',src_vocab_size)\n",
    "print('target 문장의 char 집합 :',tar_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2292f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a88efa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, 'ï': 77, '’': 78, '€': 79}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Ç': 82, 'É': 83, 'Ê': 84, 'Ô': 85, 'à': 86, 'â': 87, 'ç': 88, 'è': 89, 'é': 90, 'ê': 91, 'ë': 92, 'î': 93, 'ï': 94, 'ô': 95, 'ù': 96, 'û': 97, 'œ': 98, '\\u2009': 99, '‘': 100, '’': 101, '\\u202f': 102, '‽': 103}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e5d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "\n",
    "# 1개의 문장\n",
    "for line in lines.src:\n",
    "  encoded_line = []\n",
    "  # 각 줄에서 1개의 char\n",
    "  for char in line:\n",
    "    # 각 char을 정수로 변환\n",
    "    encoded_line.append(src_to_index[char])\n",
    "  encoder_input.append(encoded_line)\n",
    "print('source 문장의 정수 인코딩 :',encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab954598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장의 정수 인코딩 : [[1, 3, 48, 52, 3, 4, 3, 2], [1, 3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [1, 3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [1, 3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [1, 3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    encoded_line.append(tar_to_index[char])\n",
    "  decoder_input.append(encoded_line)\n",
    "print('target 문장의 정수 인코딩 :',decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1bd11f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장 레이블의 정수 인코딩 : [[3, 48, 52, 3, 4, 3, 2], [3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "  timestep = 0\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    if timestep > 0:\n",
    "      encoded_line.append(tar_to_index[char])\n",
    "    timestep = timestep + 1\n",
    "  decoder_target.append(encoded_line)\n",
    "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0405c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 최대 길이 : 22\n",
      "target 문장의 최대 길이 : 76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print('source 문장의 최대 길이 :',max_src_len)\n",
    "print('target 문장의 최대 길이 :',max_tar_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0138c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "059ae007",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b062ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79b9cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs=Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm= LSTM(units=256, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "encoder_states =[state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba87e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88d3bfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "750/750 [==============================] - 232s 305ms/step - loss: 0.7244 - val_loss: 0.6335\n",
      "Epoch 2/40\n",
      "750/750 [==============================] - 259s 346ms/step - loss: 0.4431 - val_loss: 0.5057\n",
      "Epoch 3/40\n",
      "750/750 [==============================] - 240s 320ms/step - loss: 0.3704 - val_loss: 0.4518\n",
      "Epoch 4/40\n",
      "750/750 [==============================] - 235s 313ms/step - loss: 0.3293 - val_loss: 0.4126\n",
      "Epoch 5/40\n",
      "750/750 [==============================] - 262s 350ms/step - loss: 0.3012 - val_loss: 0.3910\n",
      "Epoch 6/40\n",
      "750/750 [==============================] - 250s 333ms/step - loss: 0.2809 - val_loss: 0.3749\n",
      "Epoch 7/40\n",
      "750/750 [==============================] - 251s 335ms/step - loss: 0.2651 - val_loss: 0.3603\n",
      "Epoch 8/40\n",
      "750/750 [==============================] - 253s 337ms/step - loss: 0.2523 - val_loss: 0.3522\n",
      "Epoch 9/40\n",
      "750/750 [==============================] - 249s 332ms/step - loss: 0.2415 - val_loss: 0.3453\n",
      "Epoch 10/40\n",
      "750/750 [==============================] - 242s 323ms/step - loss: 0.2322 - val_loss: 0.3400\n",
      "Epoch 11/40\n",
      "750/750 [==============================] - 250s 334ms/step - loss: 0.2241 - val_loss: 0.3373\n",
      "Epoch 12/40\n",
      "750/750 [==============================] - 243s 324ms/step - loss: 0.2167 - val_loss: 0.3351\n",
      "Epoch 13/40\n",
      "750/750 [==============================] - 247s 330ms/step - loss: 0.2101 - val_loss: 0.3353\n",
      "Epoch 14/40\n",
      "750/750 [==============================] - 265s 353ms/step - loss: 0.2043 - val_loss: 0.3301\n",
      "Epoch 15/40\n",
      "750/750 [==============================] - 282s 376ms/step - loss: 0.1989 - val_loss: 0.3318\n",
      "Epoch 16/40\n",
      "750/750 [==============================] - 281s 374ms/step - loss: 0.1937 - val_loss: 0.3305\n",
      "Epoch 17/40\n",
      "750/750 [==============================] - 268s 358ms/step - loss: 0.1891 - val_loss: 0.3325\n",
      "Epoch 18/40\n",
      "750/750 [==============================] - 265s 353ms/step - loss: 0.1848 - val_loss: 0.3324\n",
      "Epoch 19/40\n",
      "750/750 [==============================] - 271s 362ms/step - loss: 0.1807 - val_loss: 0.3319\n",
      "Epoch 20/40\n",
      "750/750 [==============================] - 279s 372ms/step - loss: 0.1769 - val_loss: 0.3338\n",
      "Epoch 21/40\n",
      "750/750 [==============================] - 260s 347ms/step - loss: 0.1732 - val_loss: 0.3363\n",
      "Epoch 22/40\n",
      "750/750 [==============================] - 262s 349ms/step - loss: 0.1697 - val_loss: 0.3381\n",
      "Epoch 23/40\n",
      "750/750 [==============================] - 271s 362ms/step - loss: 0.1666 - val_loss: 0.3389\n",
      "Epoch 24/40\n",
      "750/750 [==============================] - 272s 362ms/step - loss: 0.1635 - val_loss: 0.3406\n",
      "Epoch 25/40\n",
      "750/750 [==============================] - 263s 350ms/step - loss: 0.1605 - val_loss: 0.3425\n",
      "Epoch 26/40\n",
      "750/750 [==============================] - 278s 371ms/step - loss: 0.1577 - val_loss: 0.3442\n",
      "Epoch 27/40\n",
      "750/750 [==============================] - 305s 407ms/step - loss: 0.1550 - val_loss: 0.3474\n",
      "Epoch 28/40\n",
      "750/750 [==============================] - 320s 427ms/step - loss: 0.1525 - val_loss: 0.3506\n",
      "Epoch 29/40\n",
      "750/750 [==============================] - 306s 409ms/step - loss: 0.1500 - val_loss: 0.3515\n",
      "Epoch 30/40\n",
      "750/750 [==============================] - 290s 386ms/step - loss: 0.1477 - val_loss: 0.3534\n",
      "Epoch 31/40\n",
      "750/750 [==============================] - 289s 385ms/step - loss: 0.1455 - val_loss: 0.3561\n",
      "Epoch 32/40\n",
      "750/750 [==============================] - 271s 361ms/step - loss: 0.1433 - val_loss: 0.3582\n",
      "Epoch 33/40\n",
      "750/750 [==============================] - 301s 401ms/step - loss: 0.1412 - val_loss: 0.3614\n",
      "Epoch 34/40\n",
      "750/750 [==============================] - 316s 421ms/step - loss: 0.1390 - val_loss: 0.3657\n",
      "Epoch 35/40\n",
      "750/750 [==============================] - 268s 357ms/step - loss: 0.1372 - val_loss: 0.3674\n",
      "Epoch 36/40\n",
      "750/750 [==============================] - 270s 360ms/step - loss: 0.1353 - val_loss: 0.3687\n",
      "Epoch 37/40\n",
      "750/750 [==============================] - 276s 368ms/step - loss: 0.1336 - val_loss: 0.3719\n",
      "Epoch 38/40\n",
      "750/750 [==============================] - 273s 364ms/step - loss: 0.1319 - val_loss: 0.3734\n",
      "Epoch 39/40\n",
      "750/750 [==============================] - 272s 362ms/step - loss: 0.1303 - val_loss: 0.3773\n",
      "Epoch 40/40\n",
      "750/750 [==============================] - 267s 357ms/step - loss: 0.1286 - val_loss: 0.3794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1af8b8e2100>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4de7c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0948743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
    "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72164514",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ded1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # 입력으로부터 인코더의 상태를 얻음\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "  target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "  target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "  stop_condition = False\n",
    "  decoded_sentence = \"\"\n",
    "\n",
    "  # stop_condition이 True가 될 때까지 루프 반복\n",
    "  while not stop_condition:\n",
    "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # 예측 결과를 문자로 변환\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "    # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "    decoded_sentence += sampled_char\n",
    "\n",
    "    # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "    if (sampled_char == '\\n' or\n",
    "        len(decoded_sentence) > max_tar_len):\n",
    "        stop_condition = True\n",
    "\n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8f05e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장: Bouge ! \n",
      "번역 문장: Va te faire gagner ! \n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장: Bonjour ! \n",
      "번역 문장: Bonjour ! \n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Got it!\n",
      "정답 문장: J'ai pigé ! \n",
      "번역 문장: Sortez ! \n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go home.\n",
      "정답 문장: Rentre à la maison. \n",
      "번역 문장: Rentre à la maison. \n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Get going.\n",
      "정답 문장: En avant. \n",
      "번역 문장: Allez-y. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "  input_seq = encoder_input[seq_index:seq_index+1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "  print(35 * \"-\")\n",
    "  print('입력 문장:', lines.src[seq_index])\n",
    "  print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "  print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d30a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6496f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수: 227815\n"
     ]
    }
   ],
   "source": [
    "lines=pd.read_csv(\"C:/Users/kimchaeyeon/Desktop/GDSC/fra.txt\", names=['src','tar','lic'], sep='\\t', encoding='UTF-8')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수:',len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68d04072",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a87bddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii(s):\n",
    "  # 프랑스어 악센트(accent) 삭제\n",
    "  # 예시 : 'déjà diné' -> deja dine\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "  # 악센트 제거 함수 호출\n",
    "  sent = to_ascii(sent.lower())\n",
    "\n",
    "  # 단어와 구두점 사이에 공백 추가.\n",
    "  # ex) \"I am a student.\" => \"I am a student .\"\n",
    "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "  # 다수 개의 공백을 하나의 공백으로 치환\n",
    "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "  return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd960bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "337ddd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "  encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "  with open(\"fra.txt\", \"r\",encoding='UTF-8') as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "      # source 데이터와 target 데이터 분리\n",
    "      src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "      # source 데이터 전처리\n",
    "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "      # target 데이터 전처리\n",
    "      tar_line = preprocess_sentence(tar_line)\n",
    "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "      encoder_input.append(src_line)\n",
    "      decoder_input.append(tar_line_in)\n",
    "      decoder_target.append(tar_line_out)\n",
    "\n",
    "      if i == num_samples - 1:\n",
    "        break\n",
    "\n",
    "  return encoder_input, decoder_input, decoder_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0dea83cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1149479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35e63984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 7)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7c530b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4481, 프랑스어 단어 집합의 크기 : 7873\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "feabdba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "18a66c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [15046 18992  2911 ... 19546 12530 13385]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "91440c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ac07ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  11,   10, 1372,    1,    0,    0,    0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[30997]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5c8c4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   14,   38, 5890,    1,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ca94376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14,   38, 5890,    1,    3,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target[30997]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c16d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(33000*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "12ba1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "69a533bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 7)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 7)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "21d00741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#기계 번역기 만들기\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "68b442bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_units = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "31216ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e85475db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 모델의 입력과 출력을 정의.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "233/233 [==============================] - 82s 316ms/step - loss: 3.4304 - acc: 0.6152 - val_loss: 2.0768 - val_acc: 0.6216\n",
      "Epoch 2/50\n",
      "233/233 [==============================] - 83s 356ms/step - loss: 1.9058 - acc: 0.6438 - val_loss: 1.7759 - val_acc: 0.7173\n",
      "Epoch 3/50\n",
      "233/233 [==============================] - 75s 323ms/step - loss: 1.6908 - acc: 0.7394 - val_loss: 1.6100 - val_acc: 0.7522\n",
      "Epoch 4/50\n",
      "233/233 [==============================] - 84s 359ms/step - loss: 1.5449 - acc: 0.7558 - val_loss: 1.4819 - val_acc: 0.7623\n",
      "Epoch 5/50\n",
      "233/233 [==============================] - 81s 347ms/step - loss: 1.4291 - acc: 0.7642 - val_loss: 1.3958 - val_acc: 0.7693\n",
      "Epoch 6/50\n",
      "233/233 [==============================] - 83s 356ms/step - loss: 1.3384 - acc: 0.7805 - val_loss: 1.3084 - val_acc: 0.7910\n",
      "Epoch 7/50\n",
      "233/233 [==============================] - 78s 336ms/step - loss: 1.2533 - acc: 0.7963 - val_loss: 1.2389 - val_acc: 0.8006\n",
      "Epoch 8/50\n",
      "233/233 [==============================] - 79s 339ms/step - loss: 1.1856 - acc: 0.8054 - val_loss: 1.1848 - val_acc: 0.8085\n",
      "Epoch 9/50\n",
      "233/233 [==============================] - 77s 331ms/step - loss: 1.1276 - acc: 0.8143 - val_loss: 1.1406 - val_acc: 0.8188\n",
      "Epoch 10/50\n",
      "233/233 [==============================] - 73s 312ms/step - loss: 1.0761 - acc: 0.8228 - val_loss: 1.0983 - val_acc: 0.8233\n",
      "Epoch 11/50\n",
      "233/233 [==============================] - 78s 336ms/step - loss: 1.0286 - acc: 0.8284 - val_loss: 1.0643 - val_acc: 0.8279\n",
      "Epoch 12/50\n",
      "233/233 [==============================] - 81s 348ms/step - loss: 0.9853 - acc: 0.8334 - val_loss: 1.0328 - val_acc: 0.8319\n",
      "Epoch 13/50\n",
      "233/233 [==============================] - 93s 399ms/step - loss: 0.9458 - acc: 0.8377 - val_loss: 1.0057 - val_acc: 0.8343\n",
      "Epoch 14/50\n",
      "233/233 [==============================] - 98s 420ms/step - loss: 0.9098 - acc: 0.8418 - val_loss: 0.9808 - val_acc: 0.8383\n",
      "Epoch 15/50\n",
      "233/233 [==============================] - 107s 459ms/step - loss: 0.8769 - acc: 0.8455 - val_loss: 0.9595 - val_acc: 0.8410\n",
      "Epoch 16/50\n",
      "233/233 [==============================] - 111s 477ms/step - loss: 0.8457 - acc: 0.8491 - val_loss: 0.9384 - val_acc: 0.8436\n",
      "Epoch 17/50\n",
      "233/233 [==============================] - 95s 410ms/step - loss: 0.8165 - acc: 0.8523 - val_loss: 0.9210 - val_acc: 0.8452\n",
      "Epoch 18/50\n",
      "233/233 [==============================] - 98s 421ms/step - loss: 0.7882 - acc: 0.8552 - val_loss: 0.9036 - val_acc: 0.8471\n",
      "Epoch 19/50\n",
      "233/233 [==============================] - 102s 439ms/step - loss: 0.7616 - acc: 0.8581 - val_loss: 0.8876 - val_acc: 0.8483\n",
      "Epoch 20/50\n",
      "233/233 [==============================] - 99s 425ms/step - loss: 0.7365 - acc: 0.8606 - val_loss: 0.8722 - val_acc: 0.8505\n",
      "Epoch 21/50\n",
      "233/233 [==============================] - 84s 359ms/step - loss: 0.7128 - acc: 0.8631 - val_loss: 0.8586 - val_acc: 0.8517\n",
      "Epoch 22/50\n",
      "233/233 [==============================] - 72s 307ms/step - loss: 0.6897 - acc: 0.8657 - val_loss: 0.8462 - val_acc: 0.8531\n",
      "Epoch 23/50\n",
      "233/233 [==============================] - 69s 296ms/step - loss: 0.6679 - acc: 0.8679 - val_loss: 0.8335 - val_acc: 0.8547\n",
      "Epoch 24/50\n",
      "233/233 [==============================] - 102s 441ms/step - loss: 0.6469 - acc: 0.8705 - val_loss: 0.8232 - val_acc: 0.8552\n",
      "Epoch 25/50\n",
      "233/233 [==============================] - 114s 488ms/step - loss: 0.6273 - acc: 0.8727 - val_loss: 0.8148 - val_acc: 0.8561\n",
      "Epoch 26/50\n",
      "233/233 [==============================] - 354s 2s/step - loss: 0.6076 - acc: 0.8750 - val_loss: 0.8032 - val_acc: 0.8577\n",
      "Epoch 27/50\n",
      "205/233 [=========================>....] - ETA: 5:14 - loss: 0.5886 - acc: 0.8772"
     ]
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # <SOS>에 해당하는 정수 생성\n",
    "  target_seq = np.zeros((1,1))\n",
    "  target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "  stop_condition = False\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  # stop_condition이 True가 될 때까지 루프 반복\n",
    "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "  while not stop_condition:\n",
    "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # 예측 결과를 단어로 변환\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "    decoded_sentence += ' '+sampled_char\n",
    "\n",
    "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "    if (sampled_char == '<eos>' or\n",
    "        len(decoded_sentence) > 50):\n",
    "        stop_condition = True\n",
    "\n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8664e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "  return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee80670",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "34ae3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37042499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 된 문장(tokens)에서 n-gram을 카운트\n",
    "def simple_count(tokens, n):\n",
    "  return Counter(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e238634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니그램 카운트 : Counter({('the',): 3, ('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('military',): 1, ('always',): 1, ('obeys',): 1, ('commands',): 1, ('of',): 1, ('party.',): 1})\n"
     ]
    }
   ],
   "source": [
    "candidate = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\n",
    "tokens = candidate.split() # 토큰화\n",
    "result = simple_count(tokens, 1) # n = 1은 유니그램\n",
    "print('유니그램 카운트 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51773c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니그램 카운트 : Counter({('the',): 7})\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "tokens = candidate.split() # 토큰화\n",
    "result = simple_count(tokens, 1) # n = 1은 유니그램\n",
    "print('유니그램 카운트 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "225b5a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니그램 카운트 : Counter({('the',): 7})\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "tokens = candidate.split() # 토큰화\n",
    "result = simple_count(tokens, 1) # n = 1은 유니그램\n",
    "print('유니그램 카운트 :',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "518de4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clip(candidate, reference_list, n):\n",
    "  # Ca 문장에서 n-gram 카운트\n",
    "  ca_cnt = simple_count(candidate, n)\n",
    "  max_ref_cnt_dict = dict()\n",
    "\n",
    "  for ref in reference_list: \n",
    "    # Ref 문장에서 n-gram 카운트\n",
    "    ref_cnt = simple_count(ref, n)\n",
    "\n",
    "    # 각 Ref 문장에 대해서 비교하여 n-gram의 최대 등장 횟수를 계산.\n",
    "    for n_gram in ref_cnt: \n",
    "      if n_gram in max_ref_cnt_dict:\n",
    "        max_ref_cnt_dict[n_gram] = max(ref_cnt[n_gram], max_ref_cnt_dict[n_gram])\n",
    "      else:\n",
    "        max_ref_cnt_dict[n_gram] = ref_cnt[n_gram]\n",
    "\n",
    "  return {\n",
    "        # count_clip = min(count, max_ref_count)\n",
    "        n_gram: min(ca_cnt.get(n_gram, 0), max_ref_cnt_dict.get(n_gram, 0)) for n_gram in ca_cnt\n",
    "     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e7b803ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보정된 유니그램 카운트 : {('the',): 2}\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "references = [\n",
    "    'the cat is on the mat',\n",
    "    'there is a cat on the mat'\n",
    "]\n",
    "result = count_clip(candidate.split(),list(map(lambda ref: ref.split(), references)),1)\n",
    "print('보정된 유니그램 카운트 :',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89909dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(candidate, reference_list, n):\n",
    "  clip_cnt = count_clip(candidate, reference_list, n) \n",
    "  total_clip_cnt = sum(clip_cnt.values()) # 분자\n",
    "\n",
    "  cnt = simple_count(candidate, n)\n",
    "  total_cnt = sum(cnt.values()) # 분모\n",
    "\n",
    "  # 분모가 0이 되는 것을 방지\n",
    "  if total_cnt == 0: \n",
    "    total_cnt = 1\n",
    "\n",
    "  # 분자 : count_clip의 합, 분모 : 단순 count의 합 ==> 보정된 정밀도\n",
    "  return (total_clip_cnt / total_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "217b05d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보정된 유니그램 정밀도 : 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "result = modified_precision(candidate.split(), list(map(lambda ref: ref.split(), references)), n=1)\n",
    "print('보정된 유니그램 정밀도 :',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bd1c6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#짧은 문장에 대한 패널티 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6f283e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ca 길이와 가장 근접한 Ref의 길이를 리턴하는 함수\n",
    "def closest_ref_length(candidate, reference_list):\n",
    "  ca_len = len(candidate) # ca 길이\n",
    "  ref_lens = (len(ref) for ref in reference_list) # Ref들의 길이\n",
    "  # 길이 차이를 최소화하는 Ref를 찾아서 Ref의 길이를 리턴\n",
    "  closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - ca_len), ref_len))\n",
    "  return closest_ref_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ed473aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate, reference_list):\n",
    "  ca_len = len(candidate)\n",
    "  ref_len = closest_ref_length(candidate, reference_list)\n",
    "\n",
    "  if ca_len > ref_len:\n",
    "    return 1\n",
    "\n",
    "  # candidate가 비어있다면 BP = 0 → BLEU = 0.0\n",
    "  elif ca_len == 0 :\n",
    "    return 0\n",
    "  else:\n",
    "    return np.exp(1 - ref_len/ca_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "65f82185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(candidate, reference_list, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "  bp = brevity_penalty(candidate, reference_list) # 브레버티 패널티, BP\n",
    "\n",
    "  p_n = [modified_precision(candidate, reference_list, n=n) for n, _ in enumerate(weights,start=1)] \n",
    "  # p1, p2, p3, ..., pn\n",
    "  score = np.sum([w_i * np.log(p_i) if p_i != 0 else 0 for w_i, p_i in zip(weights, p_n)])\n",
    "  return bp * np.exp(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ec2587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실습 코드의 BLEU : 0.5045666840058485\n",
      "패키지 NLTK의 BLEU : 0.5045666840058485\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "references = [\n",
    "    'It is a guide to action that ensures that the military will forever heed Party commands',\n",
    "    'It is the guiding principle which guarantees the military forces always being under the command of the Party',\n",
    "    'It is the practical guide for the army always to heed the directions of the party'\n",
    "]\n",
    "\n",
    "print('실습 코드의 BLEU :',bleu_score(candidate.split(),list(map(lambda ref: ref.split(), references))))\n",
    "print('패키지 NLTK의 BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), references)),candidate.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbec8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
