{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540d808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 227815\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30891</th>\n",
       "      <td>Who broke the cup?</td>\n",
       "      <td>Qui a cassé la tasse ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>I love soup.</td>\n",
       "      <td>J'adore la soupe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34632</th>\n",
       "      <td>I've caught a cold.</td>\n",
       "      <td>J'ai contracté un rhume.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181</th>\n",
       "      <td>Rinse with warm water.</td>\n",
       "      <td>Rincer avec de l'eau tiède.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11075</th>\n",
       "      <td>How's your job?</td>\n",
       "      <td>Comment va ton boulot ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870</th>\n",
       "      <td>I have to win.</td>\n",
       "      <td>Il faut que je gagne.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10300</th>\n",
       "      <td>A beer, please.</td>\n",
       "      <td>Une bière, s'il vous plaît !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14767</th>\n",
       "      <td>Find me a glass.</td>\n",
       "      <td>Trouvez-moi un verre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39900</th>\n",
       "      <td>He parties too much.</td>\n",
       "      <td>Il fait trop la bamboula.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48063</th>\n",
       "      <td>I did that on Monday.</td>\n",
       "      <td>J'ai fait ça lundi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                           tar\n",
       "30891      Who broke the cup?        Qui a cassé la tasse ?\n",
       "3363             I love soup.             J'adore la soupe.\n",
       "34632     I've caught a cold.      J'ai contracté un rhume.\n",
       "59181  Rinse with warm water.   Rincer avec de l'eau tiède.\n",
       "11075         How's your job?       Comment va ton boulot ?\n",
       "7870           I have to win.         Il faut que je gagne.\n",
       "10300         A beer, please.  Une bière, s'il vous plaît !\n",
       "14767        Find me a glass.         Trouvez-moi un verre.\n",
       "39900    He parties too much.     Il fait trop la bamboula.\n",
       "48063   I did that on Monday.           J'ai fait ça lundi."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib3\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "lines = pd.read_csv(\"C:/Users/zrowx/Downloads/fra-eng/fra.txt\", names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수 :',len(lines))\n",
    "\n",
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000] # 6만개만 저장\n",
    "lines.sample(10)\n",
    "\n",
    "# 60,000개의 샘플만 가지고 기계 번역기 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f451195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48336</th>\n",
       "      <td>I hard-boiled an egg.</td>\n",
       "      <td>\\t J'ai cuit un œuf dur. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29333</th>\n",
       "      <td>They had a choice.</td>\n",
       "      <td>\\t Elles avaient le choix. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28977</th>\n",
       "      <td>That sounds scary.</td>\n",
       "      <td>\\t Cela semble effrayant. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12624</th>\n",
       "      <td>She has a book.</td>\n",
       "      <td>\\t Elle a un livre. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21081</th>\n",
       "      <td>I understand Tom.</td>\n",
       "      <td>\\t Je comprends Tom. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59257</th>\n",
       "      <td>She has gone shopping.</td>\n",
       "      <td>\\t Elle est allée faire les courses. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>I saw him.</td>\n",
       "      <td>\\t Je le vis. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38335</th>\n",
       "      <td>You owe me a favor.</td>\n",
       "      <td>\\t Tu me dois une faveur. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8157</th>\n",
       "      <td>I was selfish.</td>\n",
       "      <td>\\t J'étais égoïste. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37832</th>\n",
       "      <td>When will it begin?</td>\n",
       "      <td>\\t Quand est-ce que ça commence ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                      tar\n",
       "48336   I hard-boiled an egg.              \\t J'ai cuit un œuf dur. \\n\n",
       "29333      They had a choice.            \\t Elles avaient le choix. \\n\n",
       "28977      That sounds scary.             \\t Cela semble effrayant. \\n\n",
       "12624         She has a book.                   \\t Elle a un livre. \\n\n",
       "21081       I understand Tom.                  \\t Je comprends Tom. \\n\n",
       "59257  She has gone shopping.  \\t Elle est allée faire les courses. \\n\n",
       "1155               I saw him.                         \\t Je le vis. \\n\n",
       "38335     You owe me a favor.             \\t Tu me dois une faveur. \\n\n",
       "8157           I was selfish.                   \\t J'étais égoïste. \\n\n",
       "37832     When will it begin?     \\t Quand est-ce que ça commence ? \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작을 의미하는 심볼 <sos> -> \\t\n",
    "# 종료를 의미하는 심볼 <eos>을 넣어주어야 함 -> \\n\n",
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)\n",
    "\n",
    "# 시작 심볼과 종료 심볼이 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64185620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 문자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e018cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 80\n",
      "target 문장의 char 집합 : 104\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print('source 문장의 char 집합 :',src_vocab_size)\n",
    "print('target 문장의 char 집합 :',tar_vocab_size)\n",
    "\n",
    "# 영어와 프랑스어는 각각 80개와 104개의 문자가 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5065b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 80\n",
      "target 문장의 char 집합 : 104\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print('source 문장의 char 집합 :',src_vocab_size)\n",
    "print('target 문장의 char 집합 :',tar_vocab_size)\n",
    "\n",
    "# 영어와 프랑스어는 각각 80개와 104개의 문자가 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8e8aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v': 1, 'a': 2, 'C': 3, 'i': 4, '7': 5, 'z': 6, '€': 7, 'c': 8, 'T': 9, ',': 10, 'G': 11, 'N': 12, '&': 13, 'Q': 14, 't': 15, 'R': 16, 'd': 17, '%': 18, '1': 19, '8': 20, 'L': 21, 'Y': 22, 'y': 23, '!': 24, 'M': 25, '/': 26, ' ': 27, 'j': 28, 'Z': 29, '\"': 30, 'X': 31, 'e': 32, 'J': 33, 'é': 34, '-': 35, 'V': 36, 'k': 37, 'E': 38, ':': 39, 'u': 40, 'B': 41, 'p': 42, 'O': 43, '?': 44, '2': 45, 'g': 46, 'b': 47, 'q': 48, 'ï': 49, 'l': 50, '3': 51, 'r': 52, 'S': 53, '4': 54, 'o': 55, '$': 56, 'A': 57, 'n': 58, 'K': 59, 'm': 60, 'x': 61, 'w': 62, 'D': 63, \"'\": 64, 'I': 65, '6': 66, '.': 67, '9': 68, 'H': 69, '5': 70, 'f': 71, 'h': 72, '0': 73, 's': 74, 'F': 75, '’': 76, 'U': 77, 'W': 78, 'P': 79}\n",
      "{'«': 1, 'v': 2, 'û': 3, 'a': 4, 'C': 5, 'i': 6, 'ê': 7, '7': 8, 'z': 9, 'c': 10, 'T': 11, ',': 12, 'G': 13, 'N': 14, 'Q': 15, '&': 16, 't': 17, 'R': 18, 'd': 19, 'î': 20, '%': 21, '1': 22, '‽': 23, '8': 24, 'L': 25, '\\u202f': 26, 'ç': 27, 'Y': 28, 'y': 29, '»': 30, '!': 31, 'M': 32, 'è': 33, 'Ô': 34, ' ': 35, 'j': 36, 'É': 37, '\"': 38, 'X': 39, 'e': 40, '\\t': 41, 'J': 42, 'é': 43, '-': 44, 'V': 45, 'k': 46, 'E': 47, 'Ê': 48, ':': 49, 'u': 50, '\\xa0': 51, 'Ç': 52, 'B': 53, 'p': 54, 'O': 55, '?': 56, '‘': 57, '2': 58, 'ë': 59, 'g': 60, 'b': 61, 'ô': 62, 'q': 63, ')': 64, 'ï': 65, 'l': 66, 'r': 67, 'â': 68, '3': 69, 'S': 70, '4': 71, 'o': 72, '$': 73, 'A': 74, 'à': 75, 'n': 76, 'K': 77, 'm': 78, 'x': 79, 'œ': 80, 'w': 81, 'D': 82, \"'\": 83, 'I': 84, '6': 85, '.': 86, '9': 87, 'H': 88, '5': 89, 'f': 90, '(': 91, 'h': 92, '0': 93, 's': 94, 'À': 95, 'F': 96, '’': 97, 'U': 98, '\\n': 99, 'W': 100, '\\u2009': 101, 'P': 102, 'ù': 103}\n"
     ]
    }
   ],
   "source": [
    "# 각 문자에 인덱스를 부여\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ac7ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 정수 인코딩 : [[11, 55, 67], [11, 55, 67], [11, 55, 67], [11, 55, 67], [69, 4, 67]]\n"
     ]
    }
   ],
   "source": [
    "# 인덱스가 부여된 문자 집합으로부터 갖고있는 훈련 데이터에 정수 인코딩 수행\n",
    "encoder_input = []\n",
    "\n",
    "# 1개의 문장\n",
    "for line in lines.src:\n",
    "    encoded_line = []\n",
    "    # 각 줄에서 1개의 char\n",
    "    for char in line:\n",
    "        # 각 char을 정수로 변환\n",
    "        encoded_line.append(src_to_index[char])\n",
    "    encoder_input.append(encoded_line)\n",
    "print('source 문장의 정수 인코딩 :',encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd049db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장의 정수 인코딩 : [[41, 35, 45, 4, 35, 31, 35, 99], [41, 35, 32, 4, 67, 10, 92, 40, 86, 35, 99], [41, 35, 47, 76, 35, 67, 72, 50, 17, 40, 35, 31, 35, 99], [41, 35, 53, 72, 50, 60, 40, 35, 31, 35, 99], [41, 35, 70, 4, 66, 50, 17, 35, 31, 35, 99]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 입력이 될 프랑스어 데이터에 대해서 정수 인코딩 수행\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    encoded_line = []\n",
    "    for char in line:\n",
    "        encoded_line.append(tar_to_index[char])\n",
    "    decoder_input.append(encoded_line)\n",
    "print('target 문장의 정수 인코딩 :',decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69dc04f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장 레이블의 정수 인코딩 : [[35, 45, 4, 35, 31, 35, 99], [35, 32, 4, 67, 10, 92, 40, 86, 35, 99], [35, 47, 76, 35, 67, 72, 50, 17, 40, 35, 31, 35, 99], [35, 53, 72, 50, 60, 40, 35, 31, 35, 99], [35, 70, 4, 66, 50, 17, 35, 31, 35, 99]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 예측값과 비교하기 위한 실제값 필요\n",
    "# 실제값에는 시작 심볼에 해당되는 <sos>가 있을 필요 없음\n",
    "# 정수 인코딩 과정에서 <sos>를 제거\n",
    "# -> 프랑스어 문장의 맨 앞에 붙어있는 '\\t'를 제거\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    timestep = 0\n",
    "    encoded_line = []\n",
    "    for char in line:\n",
    "        if timestep > 0:\n",
    "            encoded_line.append(tar_to_index[char])\n",
    "        timestep = timestep + 1\n",
    "    decoder_target.append(encoded_line)\n",
    "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])\n",
    "# decoder_input에서는 모든 문장의 앞에 붙어있던 숫자 1이 decoder_target에서는 제거됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3eb9fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 최대 길이 : 22\n",
      "target 문장의 최대 길이 : 76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print('source 문장의 최대 길이 :',max_src_len)\n",
    "print('target 문장의 최대 길이 :',max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc1ab988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 데이터의 샘플은 전부 길이가 22이 되도록 패딩\n",
    "# 프랑스어 데이터의 샘플은 전부 길이가 76이 되도록 패딩\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3001363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 값에 대해서 원-핫 인코딩을 수행\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b6ab007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "# return_state=True로 설정 -> 인코더에 입력을 넣으면 내부 상태를 리턴\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "\n",
    "# encoder_outputs은 여기서는 불필요\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c96809",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73408ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "750/750 [==============================] - 185s 244ms/step - loss: 0.7267 - val_loss: 0.6295\n",
      "Epoch 2/40\n",
      "750/750 [==============================] - 201s 268ms/step - loss: 0.4459 - val_loss: 0.5088\n",
      "Epoch 3/40\n",
      "750/750 [==============================] - 185s 247ms/step - loss: 0.3720 - val_loss: 0.4496\n",
      "Epoch 4/40\n",
      "750/750 [==============================] - 179s 239ms/step - loss: 0.3308 - val_loss: 0.4136\n",
      "Epoch 5/40\n",
      "750/750 [==============================] - 178s 238ms/step - loss: 0.3029 - val_loss: 0.3903\n",
      "Epoch 6/40\n",
      "750/750 [==============================] - 184s 245ms/step - loss: 0.2824 - val_loss: 0.3745\n",
      "Epoch 7/40\n",
      "750/750 [==============================] - 177s 237ms/step - loss: 0.2663 - val_loss: 0.3594\n",
      "Epoch 8/40\n",
      "750/750 [==============================] - 175s 233ms/step - loss: 0.2532 - val_loss: 0.3523\n",
      "Epoch 9/40\n",
      "750/750 [==============================] - 177s 236ms/step - loss: 0.2423 - val_loss: 0.3462\n",
      "Epoch 10/40\n",
      "750/750 [==============================] - 177s 236ms/step - loss: 0.2329 - val_loss: 0.3387\n",
      "Epoch 11/40\n",
      "750/750 [==============================] - 186s 248ms/step - loss: 0.2247 - val_loss: 0.3352\n",
      "Epoch 12/40\n",
      "750/750 [==============================] - 181s 242ms/step - loss: 0.2174 - val_loss: 0.3326\n",
      "Epoch 13/40\n",
      "750/750 [==============================] - 180s 240ms/step - loss: 0.2109 - val_loss: 0.3310\n",
      "Epoch 14/40\n",
      "750/750 [==============================] - 183s 244ms/step - loss: 0.2049 - val_loss: 0.3312\n",
      "Epoch 15/40\n",
      "750/750 [==============================] - 188s 250ms/step - loss: 0.1994 - val_loss: 0.3286\n",
      "Epoch 16/40\n",
      "750/750 [==============================] - 185s 247ms/step - loss: 0.1943 - val_loss: 0.3279\n",
      "Epoch 17/40\n",
      "750/750 [==============================] - 190s 253ms/step - loss: 0.1896 - val_loss: 0.3285\n",
      "Epoch 18/40\n",
      "750/750 [==============================] - 189s 252ms/step - loss: 0.1852 - val_loss: 0.3293\n",
      "Epoch 19/40\n",
      "750/750 [==============================] - 190s 253ms/step - loss: 0.1811 - val_loss: 0.3293\n",
      "Epoch 20/40\n",
      "750/750 [==============================] - 189s 252ms/step - loss: 0.1771 - val_loss: 0.3296\n",
      "Epoch 21/40\n",
      "750/750 [==============================] - 192s 257ms/step - loss: 0.1734 - val_loss: 0.3329\n",
      "Epoch 22/40\n",
      "750/750 [==============================] - 195s 260ms/step - loss: 0.1698 - val_loss: 0.3338\n",
      "Epoch 23/40\n",
      "750/750 [==============================] - 194s 259ms/step - loss: 0.1666 - val_loss: 0.3350\n",
      "Epoch 24/40\n",
      "750/750 [==============================] - 196s 261ms/step - loss: 0.1635 - val_loss: 0.3359\n",
      "Epoch 25/40\n",
      "750/750 [==============================] - 197s 263ms/step - loss: 0.1603 - val_loss: 0.3379\n",
      "Epoch 26/40\n",
      "750/750 [==============================] - 199s 265ms/step - loss: 0.1575 - val_loss: 0.3396\n",
      "Epoch 27/40\n",
      "750/750 [==============================] - 202s 269ms/step - loss: 0.1547 - val_loss: 0.3415\n",
      "Epoch 28/40\n",
      "750/750 [==============================] - 206s 274ms/step - loss: 0.1521 - val_loss: 0.3453\n",
      "Epoch 29/40\n",
      "750/750 [==============================] - 205s 274ms/step - loss: 0.1497 - val_loss: 0.3485\n",
      "Epoch 30/40\n",
      "750/750 [==============================] - 205s 274ms/step - loss: 0.1472 - val_loss: 0.3495\n",
      "Epoch 31/40\n",
      "750/750 [==============================] - 208s 277ms/step - loss: 0.1449 - val_loss: 0.3510\n",
      "Epoch 32/40\n",
      "750/750 [==============================] - 206s 275ms/step - loss: 0.1427 - val_loss: 0.3540\n",
      "Epoch 33/40\n",
      "750/750 [==============================] - 209s 278ms/step - loss: 0.1406 - val_loss: 0.3568\n",
      "Epoch 34/40\n",
      "750/750 [==============================] - 208s 277ms/step - loss: 0.1387 - val_loss: 0.3586\n",
      "Epoch 35/40\n",
      "750/750 [==============================] - 211s 281ms/step - loss: 0.1367 - val_loss: 0.3612\n",
      "Epoch 36/40\n",
      "750/750 [==============================] - 212s 282ms/step - loss: 0.1349 - val_loss: 0.3637\n",
      "Epoch 37/40\n",
      "750/750 [==============================] - 216s 289ms/step - loss: 0.1330 - val_loss: 0.3664\n",
      "Epoch 38/40\n",
      "750/750 [==============================] - 217s 290ms/step - loss: 0.1313 - val_loss: 0.3701\n",
      "Epoch 39/40\n",
      "750/750 [==============================] - 225s 300ms/step - loss: 0.1295 - val_loss: 0.3730\n",
      "Epoch 40/40\n",
      "750/750 [==============================] - 229s 305ms/step - loss: 0.1280 - val_loss: 0.3746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1526ac44f70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86ee2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f70db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 정의\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
    "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "263a88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스로부터 단어를 얻을 수 있는 index_to_src와 index_to_tar를 만들기\n",
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e71cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "            len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b033b2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 342ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장: Bouge ! \n",
      "번역 문장: Cassez-vous. \n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장: Bonjour ! \n",
      "번역 문장: Salut. \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Got it!\n",
      "정답 문장: J'ai pigé ! \n",
      "번역 문장: Compres ! \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go home.\n",
      "정답 문장: Rentre à la maison. \n",
      "번역 문장: Rentrez chez vous. \n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Get going.\n",
      "정답 문장: En avant. \n",
      "번역 문장: En route. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb64d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 Word-Level 번역기 만들기(Neural Machine Translation (seq2seq) Tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd86a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d0d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a80dde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def unicode_to_ascii(s):\n",
    "  # 프랑스어 악센트(accent) 삭제\n",
    "  # 예시 : 'déjà diné' -> deja dine\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    # 악센트 제거 함수 호출\n",
    "    sent = to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백 추가.\n",
    "    # ex) \"I am a student.\" => \"I am a student .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    # 다수 개의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deba0ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21e16de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교사 강요(Teacher Forcing)\n",
    "# 레이블에 해당되는 출력 시퀀스를 따로 분리하여 저장\n",
    "# 입력 시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가\n",
    "# 출력 시퀀스에는 종료를 의미하는 토큰인 <eos>를 추가\n",
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"C:/Users/zrowx/Downloads/fra-eng/fra.txt\", \"r\", encoding='UTF8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0194f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1cadda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 토크나이저를 통해 단어 집합을 생성\n",
    "# 정수 인코딩을 진행\n",
    "# 패딩을 진행\n",
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2dcc154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 7)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n",
    "\n",
    "# 영어 문장의 길이는 8-----------------------------------------------------------------------------------------------------------\n",
    "# 프랑스어 문장의 길이는 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcfcf37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4481, 프랑스어 단어 집합의 크기 : 7873\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7752588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 만들어줌\n",
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03d17c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [30307 25711 31026 ... 25249 25944 32152]\n"
     ]
    }
   ],
   "source": [
    "# 순서가 섞인 정수 시퀀스 리스트\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1be7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac550b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 341   3   9 253   1   0]\n",
      "[  2   4   9 100  32 265   1   0   0   0   0   0   0   0   0   0]\n",
      "[  4   9 100  32 265   1   3   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[30997])\n",
    "print(decoder_input[30997])\n",
    "print(decoder_target[30997])\n",
    "\n",
    "# decoder_target은 데이터의 구조상으로 앞에 붙은 <sos> 토큰과 뒤에 붙은 <eos>을 제외하면 동일한 정수 시퀀스를 가져야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b52b7033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터의 10%를 테스트 데이터로 분리\n",
    "n_of_val = int(33000*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e20575ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31ebc192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 7)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 7)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "929c50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a43c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64로 사용\n",
    "embedding_dim = 64\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa7f5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b50d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 모델의 입력과 출력을 정의.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19f3fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "233/233 [==============================] - 59s 235ms/step - loss: 3.2922 - acc: 0.6187 - val_loss: 2.0235 - val_acc: 0.6192\n",
      "Epoch 2/50\n",
      "233/233 [==============================] - 63s 270ms/step - loss: 1.8461 - acc: 0.6749 - val_loss: 1.7491 - val_acc: 0.7362\n",
      "Epoch 3/50\n",
      "233/233 [==============================] - 61s 262ms/step - loss: 1.6531 - acc: 0.7432 - val_loss: 1.6096 - val_acc: 0.7443\n",
      "Epoch 4/50\n",
      "233/233 [==============================] - 61s 261ms/step - loss: 1.5239 - acc: 0.7566 - val_loss: 1.4936 - val_acc: 0.7589\n",
      "Epoch 5/50\n",
      "233/233 [==============================] - 61s 264ms/step - loss: 1.4133 - acc: 0.7682 - val_loss: 1.3994 - val_acc: 0.7729\n",
      "Epoch 6/50\n",
      "233/233 [==============================] - 61s 260ms/step - loss: 1.3168 - acc: 0.7860 - val_loss: 1.3183 - val_acc: 0.7911\n",
      "Epoch 7/50\n",
      "233/233 [==============================] - 61s 263ms/step - loss: 1.2400 - acc: 0.7988 - val_loss: 1.2606 - val_acc: 0.8017\n",
      "Epoch 8/50\n",
      "233/233 [==============================] - 59s 255ms/step - loss: 1.1785 - acc: 0.8089 - val_loss: 1.2127 - val_acc: 0.8081\n",
      "Epoch 9/50\n",
      "233/233 [==============================] - 61s 261ms/step - loss: 1.1240 - acc: 0.8158 - val_loss: 1.1675 - val_acc: 0.8133\n",
      "Epoch 10/50\n",
      "233/233 [==============================] - 59s 255ms/step - loss: 1.0733 - acc: 0.8226 - val_loss: 1.1273 - val_acc: 0.8195\n",
      "Epoch 11/50\n",
      "233/233 [==============================] - 60s 260ms/step - loss: 1.0259 - acc: 0.8286 - val_loss: 1.0918 - val_acc: 0.8237\n",
      "Epoch 12/50\n",
      "233/233 [==============================] - 63s 268ms/step - loss: 0.9826 - acc: 0.8337 - val_loss: 1.0582 - val_acc: 0.8285\n",
      "Epoch 13/50\n",
      "233/233 [==============================] - 63s 271ms/step - loss: 0.9419 - acc: 0.8383 - val_loss: 1.0277 - val_acc: 0.8314\n",
      "Epoch 14/50\n",
      "233/233 [==============================] - 60s 255ms/step - loss: 0.9034 - acc: 0.8428 - val_loss: 1.0004 - val_acc: 0.8350\n",
      "Epoch 15/50\n",
      "233/233 [==============================] - 59s 254ms/step - loss: 0.8676 - acc: 0.8465 - val_loss: 0.9761 - val_acc: 0.8369\n",
      "Epoch 16/50\n",
      "233/233 [==============================] - 59s 254ms/step - loss: 0.8338 - acc: 0.8503 - val_loss: 0.9577 - val_acc: 0.8393\n",
      "Epoch 17/50\n",
      "233/233 [==============================] - 59s 254ms/step - loss: 0.8042 - acc: 0.8536 - val_loss: 0.9353 - val_acc: 0.8420\n",
      "Epoch 18/50\n",
      "233/233 [==============================] - 60s 256ms/step - loss: 0.7750 - acc: 0.8567 - val_loss: 0.9191 - val_acc: 0.8445\n",
      "Epoch 19/50\n",
      "233/233 [==============================] - 62s 267ms/step - loss: 0.7485 - acc: 0.8594 - val_loss: 0.9010 - val_acc: 0.8456\n",
      "Epoch 20/50\n",
      "233/233 [==============================] - 61s 261ms/step - loss: 0.7224 - acc: 0.8622 - val_loss: 0.8895 - val_acc: 0.8470\n",
      "Epoch 21/50\n",
      "233/233 [==============================] - 64s 274ms/step - loss: 0.6986 - acc: 0.8648 - val_loss: 0.8724 - val_acc: 0.8491\n",
      "Epoch 22/50\n",
      "233/233 [==============================] - 61s 263ms/step - loss: 0.6748 - acc: 0.8671 - val_loss: 0.8596 - val_acc: 0.8500\n",
      "Epoch 23/50\n",
      "233/233 [==============================] - 61s 261ms/step - loss: 0.6532 - acc: 0.8697 - val_loss: 0.8468 - val_acc: 0.8513\n",
      "Epoch 24/50\n",
      "233/233 [==============================] - 59s 254ms/step - loss: 0.6318 - acc: 0.8723 - val_loss: 0.8366 - val_acc: 0.8527\n",
      "Epoch 25/50\n",
      "233/233 [==============================] - 74s 318ms/step - loss: 0.6117 - acc: 0.8748 - val_loss: 0.8279 - val_acc: 0.8537\n",
      "Epoch 26/50\n",
      "233/233 [==============================] - 63s 272ms/step - loss: 0.5930 - acc: 0.8769 - val_loss: 0.8175 - val_acc: 0.8542\n",
      "Epoch 27/50\n",
      "233/233 [==============================] - 65s 280ms/step - loss: 0.5742 - acc: 0.8792 - val_loss: 0.8095 - val_acc: 0.8556\n",
      "Epoch 28/50\n",
      "233/233 [==============================] - 61s 261ms/step - loss: 0.5568 - acc: 0.8814 - val_loss: 0.8036 - val_acc: 0.8560\n",
      "Epoch 29/50\n",
      "233/233 [==============================] - 60s 259ms/step - loss: 0.5399 - acc: 0.8837 - val_loss: 0.7938 - val_acc: 0.8577\n",
      "Epoch 30/50\n",
      "233/233 [==============================] - 60s 258ms/step - loss: 0.5234 - acc: 0.8863 - val_loss: 0.7867 - val_acc: 0.8579\n",
      "Epoch 31/50\n",
      "233/233 [==============================] - 61s 261ms/step - loss: 0.5080 - acc: 0.8882 - val_loss: 0.7827 - val_acc: 0.8591\n",
      "Epoch 32/50\n",
      "233/233 [==============================] - 67s 289ms/step - loss: 0.4935 - acc: 0.8904 - val_loss: 0.7761 - val_acc: 0.8598\n",
      "Epoch 33/50\n",
      "233/233 [==============================] - 66s 282ms/step - loss: 0.4793 - acc: 0.8927 - val_loss: 0.7699 - val_acc: 0.8600\n",
      "Epoch 34/50\n",
      "233/233 [==============================] - 60s 259ms/step - loss: 0.4677 - acc: 0.8946 - val_loss: 0.7620 - val_acc: 0.8613\n",
      "Epoch 35/50\n",
      "233/233 [==============================] - 60s 259ms/step - loss: 0.4514 - acc: 0.8976 - val_loss: 0.7556 - val_acc: 0.8624\n",
      "Epoch 36/50\n",
      "233/233 [==============================] - 60s 258ms/step - loss: 0.4391 - acc: 0.8994 - val_loss: 0.7534 - val_acc: 0.8633\n",
      "Epoch 37/50\n",
      "233/233 [==============================] - 60s 258ms/step - loss: 0.4264 - acc: 0.9019 - val_loss: 0.7495 - val_acc: 0.8643\n",
      "Epoch 38/50\n",
      "233/233 [==============================] - 60s 259ms/step - loss: 0.4154 - acc: 0.9037 - val_loss: 0.7483 - val_acc: 0.8648\n",
      "Epoch 39/50\n",
      "233/233 [==============================] - 60s 256ms/step - loss: 0.4033 - acc: 0.9059 - val_loss: 0.7438 - val_acc: 0.8657\n",
      "Epoch 40/50\n",
      "233/233 [==============================] - 60s 258ms/step - loss: 0.3926 - acc: 0.9078 - val_loss: 0.7395 - val_acc: 0.8661\n",
      "Epoch 41/50\n",
      "233/233 [==============================] - 62s 265ms/step - loss: 0.3818 - acc: 0.9097 - val_loss: 0.7394 - val_acc: 0.8666\n",
      "Epoch 42/50\n",
      "233/233 [==============================] - 61s 261ms/step - loss: 0.3737 - acc: 0.9113 - val_loss: 0.7357 - val_acc: 0.8669\n",
      "Epoch 43/50\n",
      "233/233 [==============================] - 61s 261ms/step - loss: 0.3625 - acc: 0.9134 - val_loss: 0.7335 - val_acc: 0.8675\n",
      "Epoch 44/50\n",
      "233/233 [==============================] - 61s 263ms/step - loss: 0.3536 - acc: 0.9151 - val_loss: 0.7321 - val_acc: 0.8682\n",
      "Epoch 45/50\n",
      "233/233 [==============================] - 75s 321ms/step - loss: 0.3444 - acc: 0.9169 - val_loss: 0.7278 - val_acc: 0.8688\n",
      "Epoch 46/50\n",
      "233/233 [==============================] - 74s 317ms/step - loss: 0.3354 - acc: 0.9187 - val_loss: 0.7283 - val_acc: 0.8684\n",
      "Epoch 47/50\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.3277 - acc: 0.9204 - val_loss: 0.7260 - val_acc: 0.8699\n",
      "Epoch 48/50\n",
      "233/233 [==============================] - 68s 293ms/step - loss: 0.3190 - acc: 0.9219 - val_loss: 0.7257 - val_acc: 0.8706\n",
      "Epoch 49/50\n",
      "233/233 [==============================] - 61s 262ms/step - loss: 0.3120 - acc: 0.9232 - val_loss: 0.7258 - val_acc: 0.8706\n",
      "Epoch 50/50\n",
      "233/233 [==============================] - 60s 258ms/step - loss: 0.3049 - acc: 0.9244 - val_loss: 0.7236 - val_acc: 0.8711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1526bcf9c10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 128개의 배치 크기로 총 50 에포크 학습\n",
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31fc6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2093d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인용 함수\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "            len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70436e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_to_src : 영어 문장에 해당하는 정수 시퀀스를 입력받음\n",
    "# 정수로부터 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환\n",
    "# seq_to_tar : 프랑스어에 해당하는 정수 시퀀스를 입력받음\n",
    "# 정수로부터 프랑스어 단어를 리턴하는 index_to_tar을 통해 프랑스어 문장으로 변환\n",
    "\n",
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0):\n",
    "            sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "    return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "            sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ce13115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "입력문장 : i ve been at home . \n",
      "정답문장 : j etais a la maison . \n",
      "번역문장 : j etais a la maison . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "입력문장 : you re talented . \n",
      "정답문장 : vous etes talentueuse . \n",
      "번역문장 : tu es talentueuse . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "입력문장 : we re not welcome . \n",
      "정답문장 : nous ne sommes pas les bienvenus . \n",
      "번역문장 : nous ne sommes pas en train de la . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "입력문장 : tom s wounded . \n",
      "정답문장 : tom est blesse . \n",
      "번역문장 : tom est blesse . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "입력문장 : i am a cook . \n",
      "정답문장 : je suis cuisinier . \n",
      "번역문장 : je suis cuisinier . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96cd4df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "입력문장 : what happens next ? \n",
      "정답문장 : et apres ? \n",
      "번역문장 : qu est ce que il se fait ? \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "입력문장 : don t be so glum . \n",
      "정답문장 : ne sois pas si morose . \n",
      "번역문장 : ne le fais pas confiance en qui tom . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "입력문장 : tom paid cash . \n",
      "정답문장 : tom a paye en liquide . \n",
      "번역문장 : tom a paye comptant . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "입력문장 : tom arrives today . \n",
      "정답문장 : tom arrive aujourd hui . \n",
      "번역문장 : tom fait parti aujourd hui . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "입력문장 : relax a moment . \n",
      "정답문장 : detends toi un instant ! \n",
      "번역문장 : detendez vous un moment ! \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b71cb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 BLEU Score(Bilingual Evaluation Understudy Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "692cc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "45203d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보정된 유니그램 정밀도 (Modified Unigram Precision) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f90da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 된 문장(tokens)에서 n-gram을 카운트\n",
    "def simple_count(tokens, n):\n",
    "    return Counter(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6991194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니그램 카운트 : Counter({('the',): 3, ('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('military',): 1, ('always',): 1, ('obeys',): 1, ('commands',): 1, ('of',): 1, ('party.',): 1})\n"
     ]
    }
   ],
   "source": [
    "candidate = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\n",
    "tokens = candidate.split() # 토큰화\n",
    "result = simple_count(tokens, 1) # n = 1은 유니그램\n",
    "print('유니그램 카운트 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3709867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니그램 카운트 : Counter({('the',): 7})\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "tokens = candidate.split() # 토큰화\n",
    "result = simple_count(tokens, 1) # n = 1은 유니그램\n",
    "print('유니그램 카운트 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "286a4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clip(candidate, reference_list, n):\n",
    "    # Ca 문장에서 n-gram 카운트\n",
    "    ca_cnt = simple_count(candidate, n)\n",
    "    max_ref_cnt_dict = dict()\n",
    "\n",
    "    for ref in reference_list: \n",
    "        # Ref 문장에서 n-gram 카운트\n",
    "        ref_cnt = simple_count(ref, n)\n",
    "\n",
    "    # 각 Ref 문장에 대해서 비교하여 n-gram의 최대 등장 횟수를 계산.\n",
    "    for n_gram in ref_cnt: \n",
    "        if n_gram in max_ref_cnt_dict:\n",
    "            max_ref_cnt_dict[n_gram] = max(ref_cnt[n_gram], max_ref_cnt_dict[n_gram])\n",
    "        else:\n",
    "            max_ref_cnt_dict[n_gram] = ref_cnt[n_gram]\n",
    "\n",
    "    return {\n",
    "        # count_clip = min(count, max_ref_count)\n",
    "        n_gram: min(ca_cnt.get(n_gram, 0), max_ref_cnt_dict.get(n_gram, 0)) for n_gram in ca_cnt\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "037c7b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보정된 유니그램 카운트 : {('the',): 1}\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "references = [\n",
    "    'the cat is on the mat',\n",
    "    'there is a cat on the mat'\n",
    "]\n",
    "result = count_clip(candidate.split(),list(map(lambda ref: ref.split(), references)),1)\n",
    "print('보정된 유니그램 카운트 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e43b288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(candidate, reference_list, n):\n",
    "    clip_cnt = count_clip(candidate, reference_list, n) \n",
    "    total_clip_cnt = sum(clip_cnt.values()) # 분자\n",
    "\n",
    "    cnt = simple_count(candidate, n)\n",
    "    total_cnt = sum(cnt.values()) # 분모\n",
    "\n",
    "    # 분모가 0이 되는 것을 방지\n",
    "    if total_cnt == 0: \n",
    "        total_cnt = 1\n",
    "\n",
    "    # 분자 : count_clip의 합, 분모 : 단순 count의 합 ==> 보정된 정밀도\n",
    "    return (total_clip_cnt / total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f6c5c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보정된 유니그램 정밀도 : 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "result = modified_precision(candidate.split(), list(map(lambda ref: ref.split(), references)), n=1)\n",
    "print('보정된 유니그램 정밀도 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbc00f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 짧은 문장 길이에 대한 패널티(Brevity Penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "714d8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ca 길이와 가장 근접한 Ref의 길이를 리턴하는 함수\n",
    "def closest_ref_length(candidate, reference_list):\n",
    "    ca_len = len(candidate) # ca 길이\n",
    "    ref_lens = (len(ref) for ref in reference_list) # Ref들의 길이\n",
    "    # 길이 차이를 최소화하는 Ref를 찾아서 Ref의 길이를 리턴\n",
    "    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - ca_len), ref_len))\n",
    "    return closest_ref_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62aee0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate, reference_list):\n",
    "    ca_len = len(candidate)\n",
    "    ref_len = closest_ref_length(candidate, reference_list)\n",
    "\n",
    "    if ca_len > ref_len:\n",
    "        return 1\n",
    "\n",
    "    # candidate가 비어있다면 BP = 0 → BLEU = 0.0\n",
    "    elif ca_len == 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return np.exp(1 - ref_len/ca_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b81f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(candidate, reference_list, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    bp = brevity_penalty(candidate, reference_list) # 브레버티 패널티, BP\n",
    "\n",
    "    p_n = [modified_precision(candidate, reference_list, n=n) for n, _ in enumerate(weights,start=1)] \n",
    "    # p1, p2, p3, ..., pn\n",
    "    score = np.sum([w_i * np.log(p_i) if p_i != 0 else 0 for w_i, p_i in zip(weights, p_n)])\n",
    "    return bp * np.exp(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "462d004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK를 사용한 BLEU 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76aacca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실습 코드의 BLEU : 0.2797821230630072\n",
      "패키지 NLTK의 BLEU : 0.5045666840058485\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "references = [\n",
    "    'It is a guide to action that ensures that the military will forever heed Party commands',\n",
    "    'It is the guiding principle which guarantees the military forces always being under the command of the Party',\n",
    "    'It is the practical guide for the army always to heed the directions of the party'\n",
    "]\n",
    "\n",
    "print('실습 코드의 BLEU :',bleu_score(candidate.split(),list(map(lambda ref: ref.split(), references))))\n",
    "print('패키지 NLTK의 BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), references)),candidate.split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
