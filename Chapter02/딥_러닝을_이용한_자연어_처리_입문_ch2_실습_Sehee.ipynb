{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9OoaNAlvEVA",
        "outputId": "20df8a5c-3205-473f-d628-0a08ea3647ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMv4JeG1paHz",
        "outputId": "7f4e17cb-0ffc-403f-99c7-648ca23db197"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 토큰화(Tokenization)"
      ],
      "metadata": {
        "id": "RsqOti3lmt5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 단어 토큰화\n",
        "- word_tokenize\n",
        "- WordPuncTokenizer\n",
        "- test_to_word_sequence\n",
        "- Penn Treebank Tokenization"
      ],
      "metadata": {
        "id": "_xsb8sRWtLIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SNY6uTlgmobZ"
      },
      "outputs": [],
      "source": [
        "# 필요 라이브러리 import\n",
        "# word_tokenize, WordPunctTokenizer 사용\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<Penn Treebank Tokenization의 규칙>\\\n",
        "규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\\\n",
        "규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.\\\n",
        "\\\n",
        "word_tokenize와 WordPunctTokenizer가 '를 다르게 전처리 하는 것을 볼 수 있다.\\\n",
        "keras의 text_to_word_sequence는 '를 따로 분리하지는 않지만 lowercase로 바꾸고 있다.\n"
      ],
      "metadata": {
        "id": "SXiIZfS8rTLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화 수행\n",
        " # 단어 토큰화 이외에도 의미가 있는 단위로 토큰화 가능\n",
        "\n",
        "sentence = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
        "\n",
        " # word_tokenize\n",
        "print('word_tokenize 토큰화:', word_tokenize(sentence))\n",
        "\n",
        " # WordPunctTokenizer\n",
        "print('WordPunctTokenizer 토큰화:', WordPunctTokenizer().tokenize(sentence))\n",
        "\n",
        "# text_to_word_sequence\n",
        "print('text_to_word_sequence 토큰화:',text_to_word_sequence(sentence))\n",
        "\n",
        "# Penn Treebank Tokenization\n",
        "print('Penn Treebank Tokenization 토큰화:',TreebankWordTokenizer().tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C42HAFpsnFS3",
        "outputId": "fa9b35bc-ca34-492e-839a-57d67e457a75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenize 토큰화: ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "WordPunctTokenizer 토큰화: ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "text_to_word_sequence 토큰화: [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
            "Penn Treebank Tokenization 토큰화: ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문장 토큰화\n",
        "문장별로 끊어서 toenization.\\\n",
        "항상 \".\", \"!\", \"?\"로 끝난다고 문장인 것은 아니다!\n",
        "적절히 학습된 tokenizer를 사용하자\n",
        "- sent_tokenize"
      ],
      "metadata": {
        "id": "Vlt3P5sTr9RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print('문장 토큰화1 :',sent_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFTMZyKrrSqr",
        "outputId": "6b71457a-23e0-4384-b7c3-12147400d7ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 마침표가 많이 들어가 있다면 어떨까?\n",
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print('문장 토큰화2 :',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNBgJS6EnFZI",
        "outputId": "387f1ad1-63c5-48e4-8005-2d1399871534"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "중간에 Ph.D라는 용어를 잘 넘기고 문장 별로 분리하고 있다."
      ],
      "metadata": {
        "id": "KS7KntWCuzn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 한국어 토큰화 by okt 와 품사 태깅\n"
      ],
      "metadata": {
        "id": "gXhRN8FAvGfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Okt(Open Korea Text)"
      ],
      "metadata": {
        "id": "ag407gLcxzBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "\n",
        "# 형태소 추출\n",
        "print(okt.morphs(text))\n",
        "\n",
        "# 품사 태깅\n",
        "print(okt.pos(text))\n",
        "\n",
        "#  명사 추출\n",
        "print(okt.nouns(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_8fAKUFvMb7",
        "outputId": "9fa59d05-c945-4506-ef3a-5a6679202d36"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥', '러닝', '자연어', '처리', '가', '재미있기는', '합니다', '.', '그런데', '문제', '는', '영어', '보다', '한국어', '로', '할', '때', '너무', '어렵습니다', '.', '이제', '해보면', '알걸', '요', '?']\n",
            "[('딥', 'Noun'), ('러닝', 'Noun'), ('자연어', 'Noun'), ('처리', 'Noun'), ('가', 'Josa'), ('재미있기는', 'Adjective'), ('합니다', 'Verb'), ('.', 'Punctuation'), ('그런데', 'Conjunction'), ('문제', 'Noun'), ('는', 'Josa'), ('영어', 'Noun'), ('보다', 'Josa'), ('한국어', 'Noun'), ('로', 'Josa'), ('할', 'Verb'), ('때', 'Noun'), ('너무', 'Adverb'), ('어렵습니다', 'Adjective'), ('.', 'Punctuation'), ('이제', 'Noun'), ('해보면', 'Verb'), ('알걸', 'Noun'), ('요', 'Josa'), ('?', 'Punctuation')]\n",
            "['딥', '러닝', '자연어', '처리', '문제', '영어', '한국어', '때', '이제', '알걸']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kkma"
      ],
      "metadata": {
        "id": "8kat9aSnxt4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "# 형태소 추출\n",
        "print(kkma.morphs(text))\n",
        "\n",
        "# 품사 태깅\n",
        "print(kkma.pos(text))\n",
        "\n",
        "#  명사 추출\n",
        "print(kkma.nouns(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63p4m7wdxr3c",
        "outputId": "f090f605-9f18-42a9-db09-88b97a84c53b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥', '러닝', '자연어', '처리', '가', '재미있', '기', '는', '하', 'ㅂ니다', '.', '그러', 'ㄴ', '데', '문제', '는', '영어', '보다', '한국어', '로', '하', 'ㄹ', '때', '너무', '어렵', '습니다', '.', '이제', '해보', '면', '알', 'ㄹ걸요', '?']\n",
            "[('딥', 'NNG'), ('러닝', 'NNG'), ('자연어', 'NNG'), ('처리', 'NNG'), ('가', 'JKS'), ('재미있', 'VA'), ('기', 'ETN'), ('는', 'JX'), ('하', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF'), ('그러', 'VV'), ('ㄴ', 'ETD'), ('데', 'NNB'), ('문제', 'NNG'), ('는', 'JX'), ('영어', 'NNG'), ('보다', 'JKM'), ('한국어', 'NNG'), ('로', 'JKM'), ('하', 'VV'), ('ㄹ', 'ETD'), ('때', 'NNG'), ('너무', 'MAG'), ('어렵', 'VA'), ('습니다', 'EFN'), ('.', 'SF'), ('이제', 'MAG'), ('해보', 'VV'), ('면', 'ECE'), ('알', 'VV'), ('ㄹ걸요', 'EFN'), ('?', 'SF')]\n",
            "['딥', '러닝', '자연어', '처리', '데', '문제', '영어', '한국어', '때']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 정제(Cleaning), 정규화(Normalization) with 정규 표현식\n",
        "- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
        "- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\n",
        "- 정규 표현식 참고: https://hamait.tistory.com/342\n",
        "책 보다는 위의 링크가 자주 사용하는 예시와 다양한 문법이 정리되어 있다."
      ],
      "metadata": {
        "id": "4uloxo9bmvEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "\n",
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbKx7a6r0ara",
        "outputId": "9c9c8f24-3b17-48a3-f2ea-7954145785fe"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
        "\n",
        "# 영어 소문자와 대문자를 제외한 모든 문자를 공백으로 치환\n",
        "preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_wDgn5d0awW",
        "outputId": "5416df34-9f02-41b1-833d-15c804111785"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"People is in SEOUL Women's  University 2023.     Ph.D Baek's   number is 200. I WANT to go my home\"\n",
        "\n",
        "# 공백 기준으로 split하기, 공백의 개수 상관 없음\n",
        "print(re.split('\\s+', text)  )\n",
        "\n",
        "# 숫자 찾기\n",
        "print(re.findall('\\d+',text)  )\n",
        "\n",
        "# 대문자 1문자 가져오기\n",
        "print(re.findall('[A-Z]',text))\n",
        "\n",
        "# 대문자 연속 4번 이상 반복하는 글자 가져오기\n",
        "print(re.findall('[A-Z]{4,}',text))\n",
        "\n",
        "# 맨 앞 문자가 대문자로 시작. 나머지 뒷 글자는 소문자로 시작하는 단어 찾기\n",
        "print(re.findall('[A-Z][a-z]+',text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBNQa69j600-",
        "outputId": "b958a64c-0dca-4e33-f114-464aec067e27"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['People', 'is', 'in', 'SEOUL', \"Women's\", 'University', '2023.', 'Ph.D', \"Baek's\", 'number', 'is', '200.', 'I', 'WANT', 'to', 'go', 'my', 'home']\n",
            "['2023', '200']\n",
            "['P', 'S', 'E', 'O', 'U', 'L', 'W', 'U', 'P', 'D', 'B', 'I', 'W', 'A', 'N', 'T']\n",
            "['SEOUL', 'WANT']\n",
            "['People', 'Women', 'University', 'Ph', 'Baek']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
        "목적: 눈으로 봤을 때는 서로 다른 단어들이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이는 것\\\n",
        " \\\n",
        "\n",
        "- 표제어 추출: 뿌리 단어 찾기\\\n",
        "ex) is, am, are -> be\n",
        "\n",
        "- 어간 추출: 어간을 추출하는 작업(의미가 있는 부분)\\\n",
        "어간: 용언이 바뀌지 않은 부분, 의미가 바뀌지 않고 고유한 부분\\\n",
        "어미: 접사 등 바뀌는 부분\n"
      ],
      "metadata": {
        "id": "AYKkjVVvmvKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 표제어 추출"
      ],
      "metadata": {
        "id": "iW0tQJIHICCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 표제어 추출\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# 형태가 적절히 보존됨\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전: ',words)\n",
        "print('표제어 추출 후: ',[lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# 단어의 품사에 대한 정보를 제공함으로써 보다 정확한 표제어를 추출 가능\n",
        "print(lemmatizer.lemmatize('dies', 'v'))\n",
        "print(lemmatizer.lemmatize('had', 'v'))\n",
        "print(lemmatizer.lemmatize('flowers', 'n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYd4a_bMAuYi",
        "outputId": "cd7abaa1-4a7e-4c4c-85c1-7e2fc62e67a7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전:  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어 추출 후:  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n",
            "die\n",
            "have\n",
            "flower\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 어간 추출"
      ],
      "metadata": {
        "id": "Fz9RcC3qID-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<어간 추출 알고리즘 in English>\n",
        "- 포터 스테머(Poter stemmer)\n",
        "- 랭커스터 스테머(Lancaster stemmer)\\\n",
        "둘 다 알고리즘이 다르기 때문에 특정 상황에 따라 성능이 갈린다. 적절한 것을 골라서 사용할 것"
      ],
      "metadata": {
        "id": "fvTBCtKtEhlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 - 스테밍\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer1 = PorterStemmer()\n",
        "stemmer2 = LancasterStemmer()\n",
        "\n",
        "# test1\n",
        "print(\"Poter Stemmer: \", stemmer1.stem('cooking'), stemmer1.stem('cookery'), stemmer1.stem('cookbooks'))\n",
        "print('Lancaster Stemmer: ', stemmer2.stem('cooking'), stemmer2.stem('cookery'), stemmer2.stem('cookbooks'))\n",
        "\n",
        "# test2\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('Poter Stemmer: ',[stemmer1.stem(w) for w in words])\n",
        "print('Lancaster Stemmer: ',[stemmer2.stem(w) for w in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4Om711MD0cq",
        "outputId": "768bd65e-3c77-492f-f672-4898d70ff6b5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poter Stemmer:  cook cookeri cookbook\n",
            "Lancaster Stemmer:  cook cookery cookbook\n",
            "Poter Stemmer:  ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "Lancaster Stemmer:  ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "\n",
        "print('Poter Stemmer: ',[stemmer1.stem(word) for word in tokenized_sentence])\n",
        "print('Lancaster Stemmer: ',[stemmer2.stem(word) for word in tokenized_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAn-Fe75EgGy",
        "outputId": "d6453fd1-b2a3-4685-89dc-980edf3f595f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poter Stemmer:  ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n",
            "Lancaster Stemmer:  ['thi', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'bil', 'bon', \"'s\", 'chest', ',', 'but', 'an', 'acc', 'cop', ',', 'complet', 'in', 'al', 'thing', '--', 'nam', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'exceiv', 'of', 'the', 'red', 'cross', 'and', 'the', 'writ', 'not', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 불용어"
      ],
      "metadata": {
        "id": "5maOc57JGVnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "분석 할 때, 의미 없는 단어를 제거하는 과정\\\n",
        "ex) I, my, me, over, 조사, 접미사 등\\\n",
        "user가 직접 지정할 수 있지만 nltk에서 이미 불용어 사전이 존재함"
      ],
      "metadata": {
        "id": "eVzGHOJvGb_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 불용어 사전"
      ],
      "metadata": {
        "id": "i8pzmfjpH-Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 영어 불용어 사전 보기\n",
        "# 불어, 독일어, 그리스어 등 23개 언어 지원, 한국어 미포함 -> konlpy 사용\n",
        "stop_words_list = stopwords.words('english')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])\n",
        "\n",
        "# 불어 불용어 사전 보기\n",
        "stop_words_list = stopwords.words('french')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keDhDEhIFnEc",
        "outputId": "920f3094-4bd1-4432-cea6-971817898744"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
            "불용어 개수 : 157\n",
            "불용어 10개 출력 : ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 불용어 제거 하기"
      ],
      "metadata": {
        "id": "yn2PWRYMH_iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "# 영어 불용어 사전 setting\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens:\n",
        "    if word not in stop_words:\n",
        "        result.append(word)\n",
        "\n",
        "print('불용어 제거 전 :',word_tokens)\n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnwT-BvPHRX4",
        "outputId": "aab87a33-6c57-452f-a391-fa77847ee554"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 한국어 불용어 사전 사용하기"
      ],
      "metadata": {
        "id": "M4StCB_5IQH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "example = \"집에 가고 싶어요. 보내주세요. 대학원 가기 싫어요. 논문 쓰기 싫어요 살려줘. 자고 싶어요 발표 준비 해야하는데 논문 아직 안 읽었다.\"\n",
        "stop_words = \"에 대학원 논문\"\n",
        "\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(example)\n",
        "\n",
        "result = [word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "# 논문과 대학원 제거^^\n",
        "print('불용어 제거 전 :',word_tokens)\n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF5yNq0cIPtH",
        "outputId": "bb76218d-b0e7-4d8c-a4a5-d9ae13900b2d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['집', '에', '가고', '싶어요', '.', '보내주세요', '.', '대학원', '가기', '싫어요', '.', '논문', '쓰기', '싫어요', '살려줘', '.', '자고', '싶어요', '발표', '준비', '해야하는데', '논문', '아직', '안', '읽었다', '.']\n",
            "불용어 제거 후 : ['집', '가고', '싶어요', '.', '보내주세요', '.', '가기', '싫어요', '.', '쓰기', '싫어요', '살려줘', '.', '자고', '싶어요', '발표', '준비', '해야하는데', '아직', '안', '읽었다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정수 및 One-hot encoding"
      ],
      "metadata": {
        "id": "Y_RXmYFyI9t3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화 -> 정수 인코딩 -> (패딩) -> one hot encoding"
      ],
      "metadata": {
        "id": "sTFvbVJSKVNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "raw_text = \"A barber is a person. a barber is good person. \\\n",
        "a barber is huge person. \\\n",
        "he Knew A Secret! The Secret He Kept is huge secret. \\\n",
        "Huge secret. His barber kept his word. \\\n",
        "a barber kept his word. His barber kept his secret. \\\n",
        "But keeping and keeping such a huge secret to himself was driving the barber crazy. \\\n",
        "the barber went up a huge mountain.\"\n",
        "\n",
        "# 문장 토큰화\n",
        "sentences = sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21_t6Ac1IwPH",
        "outputId": "8300413c-8e9d-4564-dc84-8ad5af34058f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "    # 단어 토큰화\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    result = []\n",
        "\n",
        "    for word in tokenized_sentence:\n",
        "       # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
        "        word = word.lower()\n",
        "         # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
        "        if word not in stop_words:\n",
        "\n",
        "          # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다. 분석적으로 의미 없는 단어일 확률이 높음\n",
        "            if len(word) > 2:\n",
        "\n",
        "              # 결과에 불용어 제거된 문장이 담김\n",
        "                result.append(word)\n",
        "\n",
        "                # 단어 빈도수를 vocab에 기록\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0\n",
        "                vocab[word] += 1\n",
        "    preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwnikFdYKd11",
        "outputId": "d849ef25-9678-4d0f-ca44-4edb7e44225c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)\n",
        "\n",
        "# 빈도순 정렬\n",
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-qo9pJkLIy1",
        "outputId": "ee6bb56a-cc0e-4ccf-b358-4ae451c53fe1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩 시작\n",
        "# 높은 빈도 수를 가진 word를 1부터 지정\n",
        "# 높은 빈도 수대로 지정하던 문장의 순서대로 지정하던 사실 상관은 없다\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (key, values) in vocab_sorted :\n",
        "    if values > 1 : # 빈도수가 작은 단어는 제외.\n",
        "        i = i + 1\n",
        "        word_to_index[key] = i\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZFDC8e_LLrz",
        "outputId": "914ec550-a790-478e-a38c-65bda5e863c6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "\n",
        "# 인덱스가 5 초과인 단어 제거\n",
        "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
        "\n",
        "# 해당 단어에 대한 인덱스 정보를 삭제\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w]\n",
        "print(word_to_index)\n",
        "\n",
        "# Out of vacab 방지, 단어 사전에 단어들이 없을 수도\n",
        "word_to_index['OOV'] = 0\n",
        "print(word_to_index)\n",
        "\n",
        "encoded_sentences = []\n",
        "for sentence in preprocessed_sentences:\n",
        "    encoded_sentence = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
        "            encoded_sentence.append(word_to_index[word])\n",
        "        except KeyError:\n",
        "            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
        "            encoded_sentence.append(word_to_index['OOV'])\n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s2d-R1zLpyW",
        "outputId": "6c3560b4-2eba-4535-c65b-7cc517a6a70e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 0}\n",
            "[[1, 5], [1, 0, 5], [1, 3, 5], [0, 2], [2, 4, 3, 2], [3, 2], [1, 4, 0], [1, 4, 0], [1, 4, 2], [0, 0, 3, 2, 0, 1, 0], [1, 0, 3, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫 인코딩\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector = [0]*(len(word_to_index))\n",
        "  index = word_to_index[word]\n",
        "  one_hot_vector[index] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "lueL7mHKMS5B"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(one_hot_encoding(\"barber\", word_to_index))\n",
        "print(one_hot_encoding(\"secret\", word_to_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GadlYDyKMtlB",
        "outputId": "a588d50f-05d9-491f-e587-aa637e377ff8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 1, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### keras version"
      ],
      "metadata": {
        "id": "5UTV_LWMNaZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "# fit_on_texts: 글자와 숫자 정보 mapping -> 이전에 봤던 딕셔너리 형태\n",
        "# preprocessed_sentences: 불용어 제거 버전\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "print('단어 집합 :',tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP5C0LDPMwTL",
        "outputId": "599a7d3d-d14c-431d-f7ee-8b4426543785"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인덱싱: texts_to_sequences\n",
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "vft1bDZpNkL_",
        "outputId": "9fb6c29d-9165-41a8-dcdf-c85308a17f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13개의 단어는 너무 많으니 단어를 5개로 줄여서 사용\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용, 현재 0은 안쓰기 때문에 5까지 하려면 +1\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "\n",
        "# 실제 적용 시기\n",
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "RdgnSL1HPhPm",
        "outputId": "aebeb422-f461-4bb9-ecb3-42993d8c33d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "print('최대 길이 :',max_len)"
      ],
      "metadata": {
        "id": "QhkRaybMP0xN",
        "outputId": "a532f69a-55c3-4065-a064-8ff7452c48fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for sentence in encoded:\n",
        "    while len(sentence) < max_len:\n",
        "        sentence.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ],
      "metadata": {
        "id": "yNkhSh2rQTxm",
        "outputId": "227c2e0a-5751-443d-f96c-37c0d70fa6a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 5, 0, 0],\n",
              "       [1, 5, 0, 0],\n",
              "       [1, 3, 5, 0],\n",
              "       [2, 0, 0, 0],\n",
              "       [2, 4, 3, 2],\n",
              "       [3, 2, 0, 0],\n",
              "       [1, 4, 0, 0],\n",
              "       [1, 4, 0, 0],\n",
              "       [1, 4, 2, 0],\n",
              "       [3, 2, 1, 0],\n",
              "       [1, 3, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫 인코딩을 할 때 반드시 패딩을 우선적으로 수행할 것!\n",
        "# 문장 길이를 맞추어 주어야 한다.\n",
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "id": "zaWgQYDuOC4D",
        "outputId": "e57b6b94-3537-4201-957c-7b03b9683e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [1. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [1. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 1. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    }
  ]
}